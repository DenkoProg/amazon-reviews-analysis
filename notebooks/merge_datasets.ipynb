{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all 6 categories into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, size, when, length\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully!\n",
      "  Version: 3.5.5\n",
      "  caseSensitive: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Reviews Local Merge\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.mergeSchema\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session created successfully!\")\n",
    "print(f\"  Version: {spark.version}\")\n",
    "print(f\"  caseSensitive: true\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Working directories created:\n",
      "  Unzipped data: data/temp_unzipped_data\n",
      "  Temporary files: data/temp_output\n",
      "  Final archives: data/final_merged_data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LOCAL_EXTRACT_PATH = \"../data/temp_unzipped_data\" \n",
    "TEMP_OUTPUT_DIR = \"../data/temp_output\"           \n",
    "FINAL_OUTPUT_DIR = \"../data/final_merged_data\"     \n",
    "\n",
    "if os.path.exists(TEMP_OUTPUT_DIR): \n",
    "    shutil.rmtree(TEMP_OUTPUT_DIR)\n",
    "    \n",
    "os.makedirs(TEMP_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Working directories created:\")\n",
    "print(f\"  Unzipped data: {LOCAL_EXTRACT_PATH}\")\n",
    "print(f\"  Temporary files: {TEMP_OUTPUT_DIR}\")\n",
    "print(f\"  Final archives: {FINAL_OUTPUT_DIR}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_safe(spark, path):\n",
    "    df = spark.read.parquet(path)\n",
    "    \n",
    "    columns = df.columns\n",
    "    if len(columns) != len(set(columns)):\n",
    "        duplicates = [col for col in set(columns) if columns.count(col) > 1]\n",
    "        print(f\"Duplicate columns found: {duplicates}\")\n",
    "        \n",
    "        unique_cols = []\n",
    "        seen = set()\n",
    "        for col in columns:\n",
    "            if col not in seen:\n",
    "                unique_cols.append(col)\n",
    "                seen.add(col)\n",
    "        \n",
    "        df = df.select(unique_cols)\n",
    "        print(f\"Duplicates removed, remaining: {len(df.columns)} columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_safe(df, target_cols):\n",
    "    available = set(df.columns)\n",
    "    to_select = [col for col in target_cols if col in available]\n",
    "    \n",
    "    if to_select:\n",
    "        return df.select(to_select)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meta_cols = [\n",
    "    \"parent_asin\", \n",
    "    \"title\", \n",
    "    \"main_category\", \n",
    "    \"average_rating\", \n",
    "    \"rating_number\", \n",
    "    \"features\", \n",
    "    \"description\", \n",
    "    \"price\", \n",
    "    \"store\"\n",
    "]\n",
    "\n",
    "target_nlp_cols = [\n",
    "    \"rating\", \n",
    "    \"title\", \n",
    "    \"text\", \n",
    "    \"verified_purchase\", \n",
    "    \"parent_asin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING AND PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "INPUT_DIR = '../data/cleaned'\n",
    "zip_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.zip')]\n",
    "\n",
    "extracted_folders = []\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    path_to_zip = os.path.join(INPUT_DIR, zip_file)\n",
    "    category_name = zip_file.replace(\"_cleaned.zip\", \"\").replace(\".zip\", \"\") \n",
    "    \n",
    "    extract_to = os.path.join(LOCAL_EXTRACT_PATH, category_name)\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    print(f\"Unzip {zip_file} --> {extract_to} ...\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        extracted_folders.append(category_name)\n",
    "        print(\"   -> OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"   !!! ERROR {zip_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs = []   \n",
    "review_dfs = [] \n",
    "\n",
    "categories = [d for d in os.listdir(LOCAL_EXTRACT_PATH) if os.path.isdir(os.path.join(LOCAL_EXTRACT_PATH, d))]\n",
    "\n",
    "for cat in categories:\n",
    "    cat_path = os.path.join(LOCAL_EXTRACT_PATH, cat)\n",
    "    \n",
    "    found_meta = glob.glob(f\"{cat_path}/**/*meta*.parquet\", recursive=True)\n",
    "    found_reviews = glob.glob(f\"{cat_path}/**/*review*.parquet\", recursive=True)\n",
    "    \n",
    "    if found_meta:\n",
    "        try:\n",
    "            path = found_meta[0]\n",
    "            print(f\"  Reading Meta: {os.path.basename(path)}\")\n",
    "            \n",
    "            df = read_parquet_safe(spark, path)\n",
    "            \n",
    "            df_clean = select_safe(df, target_meta_cols)\n",
    "            \n",
    "            if df_clean is not None:\n",
    "                df_clean = df_clean.withColumn(\"main_category_label\", lit(cat))\n",
    "                meta_dfs.append(df_clean)\n",
    "                print(f\"Meta: {df_clean.count()} records\")\n",
    "            else:\n",
    "                print(f\"Meta: no columns needed\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR Meta {cat}: {e}\")\n",
    "\n",
    "    if found_reviews:\n",
    "        try:\n",
    "            path = sorted(found_reviews, key=len)[0]\n",
    "            if \"part-\" in path: \n",
    "                path = os.path.dirname(path)\n",
    "                \n",
    "            print(f\"  Reading Reviews: {os.path.basename(path)}\")\n",
    "            \n",
    "            df = read_parquet_safe(spark, path)\n",
    "            \n",
    "            df_clean = select_safe(df, target_nlp_cols)\n",
    "            \n",
    "            if df_clean is not None:\n",
    "                df_clean = df_clean.withColumn(\"category_label\", lit(cat))\n",
    "                review_dfs.append(df_clean)\n",
    "                print(f\"Reviews: {df_clean.count()} records\")\n",
    "            else:\n",
    "                print(f\"Reviews: no columns needed\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR Reviews {cat}: {e}\")\n",
    "    else:\n",
    "        print(f\" NOT FOUND Reviews {cat}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if meta_dfs:\n",
    "    print(f\"\\nMerging {len(meta_dfs)} Meta datasets...\")\n",
    "    full_meta = reduce(lambda x, y: x.unionByName(y, allowMissingColumns=True), meta_dfs)\n",
    "    \n",
    "    if \"price\" in full_meta.columns:\n",
    "        full_meta = full_meta.withColumn(\"price\", col(\"price\").cast(\"float\"))\n",
    "        full_meta = full_meta.filter(col(\"price\").isNotNull() & (col(\"price\") > 0)) \\\n",
    "                             .dropDuplicates([\"parent_asin\"])\n",
    "\n",
    "    print(f\"Saving Regression data ({full_meta.count()} rows)...\")\n",
    "    reg_path = f\"{TEMP_OUTPUT_DIR}/regression_price\"\n",
    "    full_meta.write.mode(\"overwrite\").parquet(reg_path)\n",
    "    \n",
    "    print(\"Archiving...\")\n",
    "    shutil.make_archive(reg_path, 'zip', reg_path)\n",
    "    shutil.copy(f\"{reg_path}.zip\", f\"{FINAL_OUTPUT_DIR}/regression_price.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if review_dfs:\n",
    "    print(f\"\\nMerging {len(review_dfs)} Review datasets...\")\n",
    "    full_reviews = reduce(lambda x, y: x.unionByName(y, allowMissingColumns=True), review_dfs)\n",
    "    \n",
    "    full_reviews = full_reviews.filter(col(\"text\").isNotNull() & (length(col(\"text\")) > 0))\n",
    "    full_reviews = full_reviews.withColumn(\"label\", \n",
    "        when(col(\"rating\") <= 2, 0)\n",
    "        .when(col(\"rating\") == 3, 1)\n",
    "        .otherwise(2)\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving Classification data ({full_reviews.count()} rows)...\")\n",
    "    nlp_path = f\"{TEMP_OUTPUT_DIR}/classification_reviews\"\n",
    "    full_reviews.write.mode(\"overwrite\").parquet(nlp_path)\n",
    "    \n",
    "    print(\"Archiving...\")\n",
    "    shutil.make_archive(nlp_path, 'zip', nlp_path)\n",
    "    shutil.copy(f\"{nlp_path}.zip\", f\"{FINAL_OUTPUT_DIR}/classification_reviews.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**META:**\n",
    "\n",
    "root\n",
    "\n",
    " |-- parent_asin: string (nullable = true)\n",
    "\n",
    " |-- title: string (nullable = true)\n",
    "\n",
    " |-- main_category: string (nullable = true)\n",
    "\n",
    " |-- average_rating: double (nullable = true)\n",
    "\n",
    " |-- rating_number: long (nullable = true)\n",
    "\n",
    " |-- features: array (nullable = true)\n",
    "\n",
    " |    |-- element: string (containsNull = true)\n",
    "\n",
    " |-- description: array (nullable = true)\n",
    "\n",
    " |    |-- element: string (containsNull = true)\n",
    "\n",
    " |-- price: float (nullable = true)\n",
    "\n",
    " |-- store: string (nullable = true)\n",
    " \n",
    " |-- main_category_label: string (nullable = false)\n",
    "\n",
    " 699,283 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REVIEWS:**\n",
    "\n",
    "root\n",
    "\n",
    " |-- rating: double (nullable = true)\n",
    "\n",
    " |-- title: string (nullable = true)\n",
    "\n",
    " |-- text: string (nullable = true)\n",
    "\n",
    " |-- verified_purchase: boolean (nullable = true)\n",
    "\n",
    " |-- parent_asin: string (nullable = true)\n",
    "\n",
    " |-- category_label: string (nullable = false)\n",
    " \n",
    " |-- label: integer (nullable = false)\n",
    "\n",
    " 35,202,489 records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit ('3.11.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1614c1b0428f9a7744170f6e726f1b37a0202d0ea9f400a7179d4fdaa8b27423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
