{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Industrial and Scientific Category - Data Exploration\n",
        "\n",
        "**Category**: Industrial and Scientific\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/andriimyrosh/Projects/amazon-reviews-analysis\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import rootutils\n",
        "\n",
        "\n",
        "rootutils.setup_root(Path.cwd(), indicator=\".project-root\", pythonpath=True)\n",
        "\n",
        "ROOT_DIR = Path(os.environ.get(\"PROJECT_ROOT\", Path.cwd()))\n",
        "print(ROOT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import rootutils\n",
        "\n",
        "\n",
        "rootutils.setup_root(Path.cwd(), indicator=\".project-root\", pythonpath=True)\n",
        "\n",
        "ROOT_DIR = Path(os.environ.get(\"PROJECT_ROOT\", Path.cwd()))\n",
        "\n",
        "REVIEWS_PATH = ROOT_DIR / \"data/raw/review_categories/Industrial_and_Scientific.jsonl\"\n",
        "METADATA_PATH = ROOT_DIR / \"data/raw/meta_categories/meta_Industrial_and_Scientific.jsonl\"\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Found Java via Homebrew: /opt/homebrew/opt/openjdk@17\n",
            "âš ï¸  Could not verify Java: stdout and stderr arguments may not be used with capture_output.\n",
            "âœ“ Added Java to PATH\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# CRITICAL: Set JAVA_HOME BEFORE importing Spark\n",
        "# This must be done in the notebook, not just in terminal\n",
        "\n",
        "# Try multiple methods to find Java\n",
        "java_home = None\n",
        "\n",
        "# Method 1: Check if already set\n",
        "if os.environ.get('JAVA_HOME'):\n",
        "    java_home = os.environ['JAVA_HOME']\n",
        "    print(f\"âœ“ JAVA_HOME already set to: {java_home}\")\n",
        "else:\n",
        "    # Method 2: Try Homebrew\n",
        "    try:\n",
        "        brew_prefix = subprocess.check_output(\n",
        "            ['brew', '--prefix', 'openjdk@17'], \n",
        "            text=True,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        ).strip()\n",
        "        if os.path.exists(brew_prefix):\n",
        "            java_home = brew_prefix\n",
        "            os.environ['JAVA_HOME'] = java_home\n",
        "            print(f\"âœ“ Found Java via Homebrew: {java_home}\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Method 3: Try common locations\n",
        "    if not java_home:\n",
        "        common_paths = [\n",
        "            '/opt/homebrew/opt/openjdk@17',\n",
        "            '/usr/local/opt/openjdk@17',\n",
        "            '/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home',\n",
        "            '/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home'\n",
        "        ]\n",
        "        for path in common_paths:\n",
        "            if os.path.exists(path):\n",
        "                java_home = path\n",
        "                os.environ['JAVA_HOME'] = java_home\n",
        "                print(f\"âœ“ Found Java at: {java_home}\")\n",
        "                break\n",
        "    \n",
        "    # Method 4: Use /usr/libexec/java_home (macOS)\n",
        "    if not java_home:\n",
        "        try:\n",
        "            java_home = subprocess.check_output(\n",
        "                ['/usr/libexec/java_home', '-v', '17'],\n",
        "                text=True,\n",
        "                stderr=subprocess.DEVNULL\n",
        "            ).strip()\n",
        "            os.environ['JAVA_HOME'] = java_home\n",
        "            print(f\"âœ“ Found Java via java_home: {java_home}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Verify Java is accessible\n",
        "if java_home:\n",
        "    java_bin = os.path.join(java_home, 'bin', 'java')\n",
        "    if os.path.exists(java_bin):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [java_bin, '-version'],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                timeout=5\n",
        "            )\n",
        "            print(f\"âœ“ Java verification successful\")\n",
        "            print(f\"  Version: {result.stdout.split(chr(10))[0] if result.stdout else 'N/A'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Could not verify Java: {e}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Java binary not found at: {java_bin}\")\n",
        "else:\n",
        "    print(\"âŒ ERROR: Could not find Java installation!\")\n",
        "    print(\"Please install Java 17: brew install openjdk@17\")\n",
        "    print(\"Or set JAVA_HOME manually in this cell:\")\n",
        "    print(\"  os.environ['JAVA_HOME'] = '/path/to/java'\")\n",
        "\n",
        "# Also add to PATH\n",
        "if java_home:\n",
        "    java_bin_dir = os.path.join(java_home, 'bin')\n",
        "    current_path = os.environ.get('PATH', '')\n",
        "    if java_bin_dir not in current_path:\n",
        "        os.environ['PATH'] = f\"{java_bin_dir}:{current_path}\"\n",
        "        print(f\"âœ“ Added Java to PATH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/12 11:26:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Spark Session created successfully!\n",
            "Spark Version: 4.0.1\n",
            "Spark App Name: AmazonReviews\n",
            "Spark Master: local[*]\n",
            "Spark UI: http://ip-192-168-0-101.eu-west-1.compute.internal:4040\n"
          ]
        }
      ],
      "source": [
        "from amazon_reviews_analysis.utils import build_spark\n",
        "\n",
        "spark = build_spark()\n",
        "\n",
        "print(\"âœ“ Spark Session created successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Spark App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART A: METADATA\n",
        "\n",
        "## Load Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Metadata: /Users/andriimyrosh/Projects/amazon-reviews-analysis/data/raw/meta_categories/meta_Industrial_and_Scientific.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 0:>                                                        (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 427,564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from src.amazon_reviews_analysis.utils import load_metadata\n",
        "\n",
        "\n",
        "print(f\"ðŸ“‚ Metadata: {METADATA_PATH}\")\n",
        "\n",
        "metadata_df = load_metadata(spark, METADATA_PATH)\n",
        "print(f\"Total records: {metadata_df.count():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema & Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SCHEMA\n",
            "================================================================================\n",
            "root\n",
            " |-- author: struct (nullable = true)\n",
            " |    |-- about: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- avatar: string (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |-- average_rating: double (nullable = true)\n",
            " |-- bought_together: string (nullable = true)\n",
            " |-- categories: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- description: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- details: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- features: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- images: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- hi_res: string (nullable = true)\n",
            " |    |    |-- large: string (nullable = true)\n",
            " |    |    |-- thumb: string (nullable = true)\n",
            " |    |    |-- variant: string (nullable = true)\n",
            " |-- main_category: string (nullable = true)\n",
            " |-- parent_asin: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- rating_number: long (nullable = true)\n",
            " |-- store: string (nullable = true)\n",
            " |-- subtitle: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- videos: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- title: string (nullable = true)\n",
            " |    |    |-- url: string (nullable = true)\n",
            " |    |    |-- user_id: string (nullable = true)\n",
            "\n",
            "\n",
            "Columns: 16\n",
            " 1. author\n",
            " 2. average_rating\n",
            " 3. bought_together\n",
            " 4. categories\n",
            " 5. description\n",
            " 6. details\n",
            " 7. features\n",
            " 8. images\n",
            " 9. main_category\n",
            "10. parent_asin\n",
            "11. price\n",
            "12. rating_number\n",
            "13. store\n",
            "14. subtitle\n",
            "15. title\n",
            "16. videos\n"
          ]
        }
      ],
      "source": [
        "print(\"SCHEMA\")\n",
        "print(\"=\" * 80)\n",
        "metadata_df.printSchema()\n",
        "\n",
        "print(f\"\\nColumns: {len(metadata_df.columns)}\")\n",
        "for idx, col_name in enumerate(metadata_df.columns, 1):\n",
        "    print(f\"{idx:2d}. {col_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------+---------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+------------------------+-----------+-----+-------------+-----------------------------+--------+--------------------------------------------------+--------------------------------------------------+\n",
            "|author|average_rating|bought_together|                                        categories|                                       description|                                           details|                                          features|                                            images|           main_category|parent_asin|price|rating_number|                        store|subtitle|                                             title|                                            videos|\n",
            "+------+--------------+---------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+------------------------+-----------+-----+-------------+-----------------------------+--------+--------------------------------------------------+--------------------------------------------------+\n",
            "|  NULL|           4.4|           NULL|[Industrial & Scientific, Food Service Equipmen...|[Round Lid fits 2-4qt StorPlus round storage co...|{Material -> Polypropylene, Color -> green, Bra...|[Tight double-sealing lids protect against spil...|[{https://m.media-amazon.com/images/I/8149rKiyr...| Industrial & Scientific| B07N83D8HC|14.27|          206|Carlisle FoodService Products|    NULL|Carlisle FoodService Products 1077108 StorPlus ...|[{want your PANTRY to be TRENDY & CLEAR & COST ...|\n",
            "|  NULL|           5.0|           NULL|[Industrial & Scientific, Occupational Health &...|[Make Sure You & Your Business are OSHA & ANSI ...|{Manufacturer -> SignMission, Brand -> SignMiss...|[EXTREMELY DURABLE: All our OSHA safety product...|[{https://m.media-amazon.com/images/I/61nsuB6JO...|         Office Products| B07T3RJCL7| 6.99|            1|                  SignMission|    NULL|OSHA Notice Signs - Do Not Open Door Must Be Op...|                                                []|\n",
            "|  NULL|           3.6|           NULL|[Industrial & Scientific, Test, Measure & Inspe...|[Measuring to 0.001 inch (no metric increments)...|{Manufacturer -> TEKTON, Part Number -> 7165, I...|[Reads 0-6 inches in 0.001 inch graduations, Me...|[{https://m.media-amazon.com/images/I/61Z3bHdB+...|Tools & Home Improvement| B000NQ4PVG| NULL|          125|                       TEKTON|    NULL|                   TEKTON 7165 6-Inch Dial Caliper|                                                []|\n",
            "|  NULL|           4.3|           NULL|[Industrial & Scientific, Industrial Electrical...|[Conforms to Marine automotive etc manufacturer...|{Connector Type -> Crimp, Contact Material -> C...|[MULTI SIZES EASY IDENTIFICATION- Total 108 pcs...|[{https://m.media-amazon.com/images/I/71aE+QzQU...|         All Electronics| B06XF9L8TT| NULL|           10|                      Conwork|    NULL|Conwork Heat Shrink Fork Terminal Connectors Ki...|                                                []|\n",
            "|  NULL|           4.6|           NULL|[Industrial & Scientific, Industrial Electrical...|[20pcs WH148 Single-Joint Potentiometer 1K B1K ...|{Package Dimensions -> 4.69 x 3.19 x 1.34 inche...|[Model: WH148 B1K Ohm, Single potentiometer, li...|[{https://m.media-amazon.com/images/I/617FIkO+v...| Industrial & Scientific| B07VJ4DNLW| 9.29|          422|                      HiLetgo|    NULL|HiLetgo 20pcs WH148 Single-Joint Potentiometer ...|[{TWTADE 64PCS Potentiometer Kit(WH148), https:...|\n",
            "+------+--------------+---------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+------------------------+-----------+-----+-------------+-----------------------------+--------+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "metadata_df.show(5, truncate=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table describes the range of products from Amazon (from the Industrial and Scientific category), including ratings, specifications, brands, images, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART B: REVIEWS\n",
        "\n",
        "## Load Reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Reviews: /Users/andriimyrosh/Projects/amazon-reviews-analysis/data/raw/review_categories/Industrial_and_Scientific.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 4:=========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 5,183,005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from src.amazon_reviews_analysis.utils import load_reviews\n",
        "\n",
        "print(f\"ðŸ“‚ Reviews: {REVIEWS_PATH}\")\n",
        "\n",
        "reviews_df = load_reviews(spark, REVIEWS_PATH)\n",
        "print(f\"Total records: {reviews_df.count():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema & Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SCHEMA\n",
            "================================================================================\n",
            "root\n",
            " |-- asin: string (nullable = true)\n",
            " |-- helpful_vote: long (nullable = true)\n",
            " |-- images: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- attachment_type: string (nullable = true)\n",
            " |    |    |-- large_image_url: string (nullable = true)\n",
            " |    |    |-- medium_image_url: string (nullable = true)\n",
            " |    |    |-- small_image_url: string (nullable = true)\n",
            " |-- parent_asin: string (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- timestamp: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- verified_purchase: boolean (nullable = true)\n",
            "\n",
            "\n",
            "Columns: 10\n",
            " 1. asin\n",
            " 2. helpful_vote\n",
            " 3. images\n",
            " 4. parent_asin\n",
            " 5. rating\n",
            " 6. text\n",
            " 7. timestamp\n",
            " 8. title\n",
            " 9. user_id\n",
            "10. verified_purchase\n"
          ]
        }
      ],
      "source": [
        "print(\"SCHEMA\")\n",
        "print(\"=\" * 80)\n",
        "reviews_df.printSchema()\n",
        "\n",
        "print(f\"\\nColumns: {len(reviews_df.columns)}\")\n",
        "for idx, col_name in enumerate(reviews_df.columns, 1):\n",
        "    print(f\"{idx:2d}. {col_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------+------+-----------+------+--------------------------------------------------+-------------+----------------------------+----------------------------+-----------------+\n",
            "|      asin|helpful_vote|images|parent_asin|rating|                                              text|    timestamp|                       title|                     user_id|verified_purchase|\n",
            "+----------+------------+------+-----------+------+--------------------------------------------------+-------------+----------------------------+----------------------------+-----------------+\n",
            "|B08C7HDF1F|           3|    []| B0BX2672L8|   5.0|These masks are great even though there is no '...|1676602453163|    Best value for the money|AG2L7H23R5LLKDKLBEF2Q3L2MVDA|             true|\n",
            "|B07BT4YLHT|           1|    []| B07BT4YLHT|   5.0|These scissors are so good they got stolen by o...|1671844170434|                   TOO good.|AG2L7H23R5LLKDKLBEF2Q3L2MVDA|             true|\n",
            "|B06XY65HCX|           0|    []| B06XY65HCX|   4.0|Good. Sensor push easier to work with but these...|1579636496378|                        Good|AGCI7FAH4GL5FI65HYLKWTMFZ2CQ|             true|\n",
            "|B01KW20EQ0|           0|    []| B01KW20EQ0|   5.0|Great ORB finish & size. Bought for our laundry...|1530556784971|                  Five Stars|AGXVBIUFLFGMVLATYXHJYL4A5Q7Q|             true|\n",
            "|B08F59NF33|           0|    []| B08N66L183|   1.0|These masks are notably thinner than other disp...|1643554243612|Only one ply - will not work|AGBFYI2DDIKXC5Y4FARTYDTQBMFQ|             true|\n",
            "+----------+------------+------+-----------+------+--------------------------------------------------+-------------+----------------------------+----------------------------+-----------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "reviews_df.show(5, truncate=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table contains information about user experiences, ratings, and comments about products. This data can be used to analyze sentiment, identify popular products, and correlate ratings with review text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA EXPLORATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MISSING VALUES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== METADATA: Missing Values Analysis ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+--------------+---------------+----------+-----------+-------+--------+------+-------------------+-----------+-----------------+-------------+--------------------+------------------+-----+------+\n",
            "|            author|average_rating|bought_together|categories|description|details|features|images|      main_category|parent_asin|            price|rating_number|               store|          subtitle|title|videos|\n",
            "+------------------+--------------+---------------+----------+-----------+-------+--------+------+-------------------+-----------+-----------------+-------------+--------------------+------------------+-----+------+\n",
            "|0.9999181409098988|           0.0|            1.0|       0.0|        0.0|    0.0|     0.0|   0.0|0.04743851212917832|        0.0|0.478522513588609|          0.0|0.008073645115117269|0.9997988605214658|  0.0|   0.0|\n",
            "+------------------+--------------+---------------+----------+-----------+-------+--------+------+-------------------+-----------+-----------------+-------------+--------------------+------------------+-----+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 14:>                                                       (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+--------------------+------------------+-------------------+\n",
            "|   categories_empty|        images_empty| description_empty|     features_empty|\n",
            "+-------------------+--------------------+------------------+-------------------+\n",
            "|0.08344247878680151|7.952025895538446E-5|0.3339991205994892|0.21496664826786166|\n",
            "+-------------------+--------------------+------------------+-------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum, when, size\n",
        "\n",
        "print(\"=== METADATA: Missing Values Analysis ===\")\n",
        "total_meta = metadata_df.count()\n",
        "\n",
        "# % missed data in each column\n",
        "nulls = metadata_df.select([\n",
        "    (sum(when(col(c).isNull(), 1).otherwise(0)) / total_meta).alias(c)\n",
        "    for c in metadata_df.columns\n",
        "])\n",
        "\n",
        "nulls.show()\n",
        "\n",
        "metadata_df.select([\n",
        "    (sum(when(size(col(c)) == 0, 1).otherwise(0)) / total_meta).alias(f\"{c}_empty\")\n",
        "    for c in [\"categories\", \"images\", \"description\", \"features\"]\n",
        "]).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "METADATA: metadata_df table has a significant number of gaps, especially in columns related to content (description, features, categories) and price.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== REVIEWS: Missing Values Analysis ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 20:========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------+------+-----------+------+----+---------+-----+-------+-----------------+\n",
            "|asin|helpful_vote|images|parent_asin|rating|text|timestamp|title|user_id|verified_purchase|\n",
            "+----+------------+------+-----------+------+----+---------+-----+-------+-----------------+\n",
            "| 0.0|         0.0|   0.0|        0.0|   0.0| 0.0|      0.0|  0.0|    0.0|              0.0|\n",
            "+----+------------+------+-----------+------+----+---------+-----+-------+-----------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"=== REVIEWS: Missing Values Analysis ===\")\n",
        "\n",
        "total_reviews = reviews_df.count()\n",
        "\n",
        "nulls = reviews_df.select([\n",
        "    (sum(when(col(c).isNull(), 1).otherwise(0)) / total_reviews).alias(c)\n",
        "    for c in reviews_df.columns\n",
        "])\n",
        "nulls.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REVIEWS: reviews_df has high data quality â€” all fields are filled in. This means the data can be used for further analysis without any problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BASIC STATISTICS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/12 11:27:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "[Stage 23:============================>                            (6 + 6) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------+---------------+--------------+--------------------+------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|summary|   average_rating|bought_together| main_category|         parent_asin|             price|    rating_number|               store|            subtitle|               title|\n",
            "+-------+-----------------+---------------+--------------+--------------------+------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "|  count|           427564|              0|        407281|              427564|            222965|           427564|              424112|                  86|              427564|\n",
            "|   mean|4.216573893031269|           NULL|          NULL|2.6673319940277777E9|60.794731433590826|95.00709835252734|4.880818106728346E10|                NULL|                 NaN|\n",
            "| stddev|0.846767308326146|           NULL|          NULL|2.8000093148834596E9|471.50593680709363|874.9025889135079|2.315902547904218...|                NULL|                 NaN|\n",
            "|    min|              1.0|           NULL|AMAZON FASHION|          0072823275|              0.01|                1|            !RAKRISA|        10th Edition|                    |\n",
            "|    max|              5.0|           NULL|   Video Games|          B42X6B8YJO|                 â€”|           201742|              ï»¿Oltec|Wall Chart â€“ Wall...|ðŸ”† Sunny Home Sil...|\n",
            "+-------+-----------------+---------------+--------------+--------------------+------------------+-----------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "metadata_df.describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "METADATA: metadata_df shows that the products are mostly positively rated, but the spread in prices and number of ratings is large.\n",
        "The price ranges indicate the presence of both budget and premium brands.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 26:====================================================>   (17 + 1) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+-----------------+-------------------+------------------+--------------------+--------------------+-------+--------------------+\n",
            "|summary|                asin|     helpful_vote|        parent_asin|            rating|                text|           timestamp|  title|             user_id|\n",
            "+-------+--------------------+-----------------+-------------------+------------------+--------------------+--------------------+-------+--------------------+\n",
            "|  count|             5183005|          5183005|            5183005|           5183005|             5183005|             5183005|5183005|             5183005|\n",
            "|   mean|2.1785267420744963E9|0.932035180363515|2.142899837661592E9| 4.183227297677699|3.104128449519329...|1.564998718237724...|    NaN|                NULL|\n",
            "| stddev| 2.224654697769768E9| 9.97075258843753| 2.21629437101025E9|1.3768442328925246|5.403323856320201E20| 8.31608136135859E10|    NaN|                NULL|\n",
            "|    min|          0072823275|               -1|         0072823275|               1.0|                    |        893573815000|      !|AE22222X4BO3JQVOM...|\n",
            "|    max|          b00bubbtto|             5611|         B42X6B8YJO|               5.0|                ðŸ«¶ðŸ¾|       1694540425306|   ðŸ«¶ðŸ˜Œ|AHZZZZPE45DYV2WZ2...|\n",
            "+-------+--------------------+-----------------+-------------------+------------------+--------------------+--------------------+-------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "reviews_df.describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REVIEWS: Most users leave positive reviews, but there are a number of low ratings. The review texts are of varying lengths, sometimes even emojis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average rating for both products and reviews is â‰ˆ 4, meaning positive ratings prevail.\n",
        "\n",
        "The data varies by price, popularity, and publication period.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EXPLORATION OF SOME COLUMNS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 29:========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+\n",
            "|rating|  count|\n",
            "+------+-------+\n",
            "|   1.0| 584133|\n",
            "|   2.0| 234763|\n",
            "|   3.0| 315792|\n",
            "|   4.0| 560932|\n",
            "|   5.0|3487385|\n",
            "+------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "reviews_df.groupBy(\"rating\").count().orderBy(\"rating\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rating Distribution:\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+\n",
            "|rating|  count|\n",
            "+------+-------+\n",
            "|   1.0| 584133|\n",
            "|   2.0| 234763|\n",
            "|   3.0| 315792|\n",
            "|   4.0| 560932|\n",
            "|   5.0|3487385|\n",
            "+------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/12 11:27:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Rating Distribution (Percentages & Cumulative):\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/12 11:27:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/11/12 11:27:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+----------+--------------+\n",
            "|rating|  count|percentage|cumulative_pct|\n",
            "+------+-------+----------+--------------+\n",
            "|   1.0| 584133|     11.27|         11.27|\n",
            "|   2.0| 234763|      4.53|         15.80|\n",
            "|   3.0| 315792|      6.09|         21.89|\n",
            "|   4.0| 560932|     10.82|         32.71|\n",
            "|   5.0|3487385|     67.29|        100.00|\n",
            "+------+-------+----------+--------------+\n",
            "\n",
            "\n",
            "Rating Distribution (Visual - 1 block = 50k reviews):\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             584,133 ( 11.3%)\n",
            "2.0: â–ˆâ–ˆâ–ˆâ–ˆ                    234,763 (  4.5%)\n",
            "3.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  315,792 (  6.1%)\n",
            "4.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             560,932 ( 10.8%)\n",
            "5.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  3,487,385 ( 67.3%)\n",
            "\n",
            "ðŸ“Š Data Cleaning Insights:\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 64:========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Low ratings (1-2 stars): 818,896 (15.8%)\n",
            "High ratings (4-5 stars): 4,048,317 (78.1%)\n",
            "Rating bias: 78.1% are 4-5 stars (typical for review data)\n",
            "\n",
            "ðŸ’¡ Cleaning Decision: Keep all ratings as they represent genuine user feedback.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "rating_counts = (\n",
        "    reviews_df.groupBy(\"rating\")\n",
        "    .count()\n",
        "    .orderBy(\"rating\")\n",
        ")\n",
        "\n",
        "# Display rating distribution table\n",
        "print(\"Rating Distribution:\")\n",
        "print(\"=\" * 50)\n",
        "rating_counts.show()\n",
        "\n",
        "# Calculate percentages for data cleaning decisions\n",
        "total_reviews = reviews_df.count()\n",
        "print(f\"\\nRating Distribution (Percentages & Cumulative):\")\n",
        "print(\"=\" * 50)\n",
        "rating_pct = rating_counts.withColumn(\n",
        "    \"percentage\", \n",
        "    (col(\"count\") / total_reviews * 100).cast(\"decimal(5,2)\")\n",
        ").withColumn(\n",
        "    \"cumulative_pct\",\n",
        "    (spark_sum(col(\"count\")).over(\n",
        "        Window.orderBy(\"rating\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "    ) / total_reviews * 100).cast(\"decimal(5,2)\")\n",
        ")\n",
        "rating_pct.show()\n",
        "\n",
        "# Text-based visualization for quick assessment\n",
        "print(\"\\nRating Distribution (Visual - 1 block = 50k reviews):\")\n",
        "print(\"=\" * 50)\n",
        "for row in rating_counts.collect():\n",
        "    rating = int(row['rating'])\n",
        "    count = row['count']\n",
        "    pct = (count / total_reviews * 100)\n",
        "    bar = 'â–ˆ' * (count // 50000)  # Scale: 1 block = 50k reviews\n",
        "    print(f\"{rating}.0: {bar:20s} {count:>10,} ({pct:>5.1f}%)\")\n",
        "\n",
        "# Data cleaning insights\n",
        "print(\"\\nðŸ“Š Data Cleaning Insights:\")\n",
        "print(\"=\" * 50)\n",
        "low_ratings = rating_counts.filter(col(\"rating\") <= 2.0).agg(spark_sum(\"count\").alias(\"low_count\")).collect()[0][\"low_count\"]\n",
        "high_ratings = rating_counts.filter(col(\"rating\") >= 4.0).agg(spark_sum(\"count\").alias(\"high_count\")).collect()[0][\"high_count\"]\n",
        "print(f\"Low ratings (1-2 stars): {low_ratings:,} ({(low_ratings/total_reviews*100):.1f}%)\")\n",
        "print(f\"High ratings (4-5 stars): {high_ratings:,} ({(high_ratings/total_reviews*100):.1f}%)\")\n",
        "print(f\"Rating bias: {(high_ratings/total_reviews*100):.1f}% are 4-5 stars (typical for review data)\")\n",
        "print(f\"\\nðŸ’¡ Cleaning Decision: Keep all ratings as they represent genuine user feedback.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 70:========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|      avg(rating)|\n",
            "+-----------------+\n",
            "|4.183227297677699|\n",
            "+-----------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "reviews_df.agg({'rating': 'avg'}).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most users give the highest rating â€” 5.0.\n",
        "\n",
        "This indicates either general user satisfaction or a bias in ratings (people are more likely to leave reviews when they are very satisfied).\n",
        "\n",
        "The fewest reviews have a rating of 2.0. Low ratings are less common, which is also a typical effect â€” dissatisfied users sometimes do not leave a review.\n",
        "\n",
        "The distribution is asymmetric â€” clearly biased towards high ratings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 73:========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------+\n",
            "|verified_purchase|  count|\n",
            "+-----------------+-------+\n",
            "|             true|4909835|\n",
            "|            false| 273170|\n",
            "+-----------------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "reviews_df.groupBy(\"verified_purchase\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most reviews (â‰ˆ90%) are verified by purchase, i.e. left by users who actually purchased the product.\n",
        "\n",
        "Only about 10% are unverified, which could mean:\n",
        "\n",
        "1. the user left the review without purchasing through another platform;\n",
        "\n",
        "2. or potentially less reliable reviews (bots, advertising, subjective opinions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 76:========================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|summary|       text_length|\n",
            "+-------+------------------+\n",
            "|  count|           5183005|\n",
            "|   mean|176.26756852443708|\n",
            "| stddev| 283.7600258038013|\n",
            "|    min|                 0|\n",
            "|    max|             33276|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import length\n",
        "\n",
        "reviews_df.select(length(\"text\").alias(\"text_length\")).describe().show() # description of review text lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Average length ~173 characters â†’ most users leave short, concise comments\n",
        "\n",
        "Large standard deviation (~247) â†’ text length varies greatly - there are both very short and extremely long reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 81:>                                                       (0 + 12) / 13]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+-------------+-----+\n",
            "|             user_id|      asin|    timestamp|count|\n",
            "+--------------------+----------+-------------+-----+\n",
            "|AGALPU5ARZEK75CGK...|B07SYB2BFW|1616840817793|   27|\n",
            "|AHK2K4QJG2LOFDO24...|B07S8K4F5J|1642106258544|   11|\n",
            "|AHJETSJDQNQDIRL66...|B01LXYM03A|1551424666757|   10|\n",
            "|AG7QXEUHBSFEUUW46...|B01C5QR4HS|1490123759000|   10|\n",
            "|AFFKTOSWUZCSSHHMT...|B076KNYCZ6|1626279666858|   10|\n",
            "+--------------------+----------+-------------+-----+\n",
            "only showing top 5 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "reviews_df.groupBy(\"user_id\", \"asin\", \"timestamp\").count().orderBy(\"count\", ascending=False).show(5) # potential duplicate or suspicious entries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same user left several (up to 10) identical entries about the same product at the same time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate groups found: 45,098\n",
            "\n",
            "Sample duplicate entries:\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+----------+-------------+-----+\n",
            "|user_id                     |asin      |timestamp    |count|\n",
            "+----------------------------+----------+-------------+-----+\n",
            "|AGALPU5ARZEK75CGKYELK232AHGA|B07SYB2BFW|1616840817793|27   |\n",
            "|AHK2K4QJG2LOFDO24VTVMLH3SDKA|B07S8K4F5J|1642106258544|11   |\n",
            "|AFFKTOSWUZCSSHHMTMYPMQJTUAKQ|B076KNYCZ6|1626279666858|10   |\n",
            "|AG7QXEUHBSFEUUW46T7ECEDJSNHQ|B01C5QR4HS|1490123759000|10   |\n",
            "|AHJETSJDQNQDIRL66CV5LO26UNUQ|B01LXYM03A|1551424666757|10   |\n",
            "|AEP3ESK7PUBKZGHW3HZKAWHQWW5A|B01AGQ3RVQ|1617291783217|9    |\n",
            "|AFI6SGYRSOTESKN26JO53BGBZB6A|B0B1HVR5LL|1670853016471|9    |\n",
            "|AGRTAIXLOKGMMKT77XPBHUQ3Y33A|B00BUF0YLO|1603564758066|9    |\n",
            "|AHYVEMREIHVJYUXFD5HPAJ5FWMLA|B0B19W7SY8|1686431145567|9    |\n",
            "|AEAP7AJTQY65MVRHKPP2MMUYDU6Q|B07MQBQ7Y5|1681771589179|9    |\n",
            "+----------------------------+----------+-------------+-----+\n",
            "only showing top 10 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Duplicate Statistics:\n",
            "  Maximum duplicates in one group: 27\n",
            "  Average duplicates per group: 2.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 109:=======================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Estimated duplicate records: 98,090\n",
            "  Percentage of data that are duplicates: 1.89%\n",
            "\n",
            "ðŸ’¡ Cleaning Decision: Remove duplicates using window function (keep first occurrence).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import count, col, sum as spark_sum\n",
        "\n",
        "# Detailed duplicate analysis for cleaning decisions\n",
        "duplicates = reviews_df.groupBy(\"user_id\", \"asin\", \"timestamp\").count().filter(\"count > 1\")\n",
        "duplicate_count = duplicates.count()\n",
        "\n",
        "print(f\"Duplicate groups found: {duplicate_count:,}\")\n",
        "\n",
        "# Show examples of duplicates\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\nSample duplicate entries:\")\n",
        "    print(\"=\" * 60)\n",
        "    duplicates.orderBy(col(\"count\").desc()).show(10, truncate=False)\n",
        "    \n",
        "    # Analyze duplicate patterns\n",
        "    max_duplicates = duplicates.agg({\"count\": \"max\"}).collect()[0][\"max(count)\"]\n",
        "    avg_duplicates = duplicates.agg({\"count\": \"avg\"}).collect()[0][\"avg(count)\"]\n",
        "    print(f\"\\nDuplicate Statistics:\")\n",
        "    print(f\"  Maximum duplicates in one group: {max_duplicates}\")\n",
        "    print(f\"  Average duplicates per group: {avg_duplicates:.2f}\")\n",
        "    \n",
        "    # Estimate total duplicate records\n",
        "    total_duplicate_records = duplicates.agg(spark_sum(\"count\")).collect()[0][\"sum(count)\"]\n",
        "    total_reviews = reviews_df.count()\n",
        "    print(f\"  Estimated duplicate records: {total_duplicate_records:,}\")\n",
        "    print(f\"  Percentage of data that are duplicates: {(total_duplicate_records/total_reviews*100):.2f}%\")\n",
        "    print(f\"\\nðŸ’¡ Cleaning Decision: Remove duplicates using window function (keep first occurrence).\")\n",
        "else:\n",
        "    print(\"âœ“ No duplicates found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 115:=======================================>               (13 + 5) / 18]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Entity Counts:\n",
            "==================================================\n",
            "Unique products (asin):        566,640\n",
            "Unique parent products:       433,651\n",
            "Unique users:                  3,603,690\n",
            "Total reviews:                 5,183,005\n",
            "\n",
            "Average Statistics:\n",
            "==================================================\n",
            "Reviews per product:            9.15\n",
            "Reviews per parent product:     11.95\n",
            "Reviews per user:               1.44\n",
            "\n",
            "ðŸ’¡ Cleaning Decision: These ratios help identify potential data quality issues.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import approx_count_distinct, count as spark_count\n",
        "\n",
        "# Unique counts for data quality assessment\n",
        "unique_stats = reviews_df.select(\n",
        "    approx_count_distinct(\"asin\").alias(\"unique_products\"),\n",
        "    approx_count_distinct(\"user_id\").alias(\"unique_users\"),\n",
        "    approx_count_distinct(\"parent_asin\").alias(\"unique_parent_products\")\n",
        ").collect()[0]\n",
        "\n",
        "total_reviews = reviews_df.count()\n",
        "\n",
        "print(\"Unique Entity Counts:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Unique products (asin):        {unique_stats['unique_products']:,}\")\n",
        "print(f\"Unique parent products:       {unique_stats['unique_parent_products']:,}\")\n",
        "print(f\"Unique users:                  {unique_stats['unique_users']:,}\")\n",
        "print(f\"Total reviews:                 {total_reviews:,}\")\n",
        "print()\n",
        "print(\"Average Statistics:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Reviews per product:            {(total_reviews/unique_stats['unique_products']):.2f}\")\n",
        "print(f\"Reviews per parent product:     {(total_reviews/unique_stats['unique_parent_products']):.2f}\")\n",
        "print(f\"Reviews per user:               {(total_reviews/unique_stats['unique_users']):.2f}\")\n",
        "print(f\"\\nðŸ’¡ Cleaning Decision: These ratios help identify potential data quality issues.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compare it to the total number of reviews, we can conclude the average reviews per user and per product.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Price Analysis ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------------------------------------------------+--------------+\n",
            "|    price|                                             title|average_rating|\n",
            "+---------+--------------------------------------------------+--------------+\n",
            "|129266.64|Senco 08S250W592 2-1/2\" x #8 Duraspin Collated ...|           3.5|\n",
            "|124024.12|Merit Glue Bond Refill for 350-RP UNSCORED, Alu...|           4.0|\n",
            "|  49999.0|JG MAKER Industrial SLA 3D Printer JG-A600 Larg...|           5.0|\n",
            "| 32999.99|On/Go One COVID-19 Rapid Antigen Home Test, 1 P...|           5.0|\n",
            "| 30769.65|                 Weight Set(20), 50 kg-1 g, ASTM 1|           5.0|\n",
            "| 25666.25|     SPX 2 JAWPULLER, 100 TON, Universal (PH1002J)|           5.0|\n",
            "| 21385.47|GOLEHS Osmium (Os) Density Cube, Laboratory-Gra...|           1.0|\n",
            "| 15334.52|Starrett 123Z-72 Vernier Caliper, Steel, Nib St...|           3.0|\n",
            "| 15183.99|OTC 1854 100-Ton Capacity Shop Press with Elect...|           5.0|\n",
            "| 14530.99|Fluke Networks DSX2-8000 CableAnalyzer Copper C...|           4.6|\n",
            "+---------+--------------------------------------------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Price Range Distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 119:>                                                      (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-----+\n",
            "|     price_range|count|\n",
            "+----------------+-----+\n",
            "|    Mid ($10-25)|83634|\n",
            "|   Budget (<$10)|54165|\n",
            "|   Luxury (>$50)|44951|\n",
            "|Premium ($25-50)|40193|\n",
            "+----------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"=== Price Analysis ===\")\n",
        "# Note: price is stored as string, need to cast for ordering\n",
        "# Use try_cast to handle invalid price values (returns NULL instead of error)\n",
        "metadata_df.filter(col(\"price\").isNotNull()) \\\n",
        "    .withColumn(\"price_double\", col(\"price\").try_cast(\"double\")) \\\n",
        "    .filter(col(\"price_double\").isNotNull()) \\\n",
        "    .select(\"price\", \"title\", \"average_rating\") \\\n",
        "    .orderBy(col(\"price_double\").desc()) \\\n",
        "    .show(10, truncate=50)\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "# Cast price to double before comparison (price is stored as string)\n",
        "# Use try_cast to handle invalid price values gracefully\n",
        "price_ranges = metadata_df.filter(col(\"price\").isNotNull()) \\\n",
        "    .withColumn(\"price_double\", col(\"price\").try_cast(\"double\")) \\\n",
        "    .filter(col(\"price_double\").isNotNull()) \\\n",
        "    .withColumn(\"price_range\",\n",
        "        when(col(\"price_double\") < 10, \"Budget (<$10)\")\n",
        "        .when(col(\"price_double\") < 25, \"Mid ($10-25)\")\n",
        "        .when(col(\"price_double\") < 50, \"Premium ($25-50)\")\n",
        "        .otherwise(\"Luxury (>$50)\")\n",
        "    ) \\\n",
        "    .groupBy(\"price_range\").count() \\\n",
        "    .orderBy(\"count\", ascending=False)\n",
        "\n",
        "print(\"\\nPrice Range Distribution:\")\n",
        "price_ranges.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data covers a very wide price range, from budget to luxury goods.\n",
        "\n",
        "The most popular price segment varies by category.\n",
        "\n",
        "A significant proportion of products across different price ranges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Price Range Statistics (Average Rating by Price Range):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 122:>                                                      (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-------------+------------------+\n",
            "|     price_range|product_count|        avg_rating|\n",
            "+----------------+-------------+------------------+\n",
            "|Premium ($25-50)|        40193|4.3402831338790415|\n",
            "|    Mid ($10-25)|        83634|4.3276215414783366|\n",
            "|   Budget (<$10)|        54165| 4.280635096464511|\n",
            "|   Luxury (>$50)|        44951| 4.266677048341532|\n",
            "+----------------+-------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "# Cast price to double before comparison (price is stored as string)\n",
        "# Use try_cast to handle invalid price values gracefully\n",
        "price_stats = metadata_df.filter(col(\"price\").isNotNull()) \\\n",
        "    .withColumn(\"price_double\", col(\"price\").try_cast(\"double\")) \\\n",
        "    .filter(col(\"price_double\").isNotNull()) \\\n",
        "    .withColumn(\"price_range\",\n",
        "        when(col(\"price_double\") < 10, \"Budget (<$10)\")\n",
        "        .when(col(\"price_double\") < 25, \"Mid ($10-25)\")\n",
        "        .when(col(\"price_double\") < 50, \"Premium ($25-50)\")\n",
        "        .otherwise(\"Luxury (>$50)\")\n",
        "    ) \\\n",
        "    .groupBy(\"price_range\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"product_count\"),\n",
        "        avg(\"average_rating\").alias(\"avg_rating\")\n",
        "    ) \\\n",
        "    .orderBy(\"avg_rating\", ascending=False)\n",
        "\n",
        "print(\"\\nPrice Range Statistics (Average Rating by Price Range):\")\n",
        "price_stats.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation Analysis (for data quality assessment):\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Products with all numeric fields: 222,943 / 427,564 (52.1%)\n",
            "\n",
            "Correlation Matrix:\n",
            "                  | Rating Number | Price\n",
            "------------------|---------------|--------\n",
            "Average Rating    |        0.0317 | -0.0202\n",
            "Rating Number     |        1.0000 | -0.0078\n",
            "Price             |       -0.0078 | 1.0000\n",
            "\n",
            "ðŸ“Š Data Cleaning Insights:\n",
            "============================================================\n",
            "â€¢ Average Rating vs Rating Number: 0.0317\n",
            "  â†’ Low correlation suggests ratings are independent of review volume\n",
            "â€¢ Average Rating vs Price: -0.0202\n",
            "  â†’ Low correlation suggests price doesn't strongly predict rating\n",
            "â€¢ Rating Number vs Price: -0.0078\n",
            "  â†’ Indicates relationship between product popularity and price\n",
            "\n",
            "ðŸ’¡ Cleaning Decision: All correlations are reasonable. No data quality issues detected.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import corr, col\n",
        "\n",
        "# Calculate correlations using Spark SQL (for data cleaning decisions)\n",
        "print(\"Correlation Analysis (for data quality assessment):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Filter out nulls and cast price to double (price is stored as string)\n",
        "metadata_with_numeric = metadata_df.filter(\n",
        "    col(\"average_rating\").isNotNull() &\n",
        "    col(\"rating_number\").isNotNull() &\n",
        "    col(\"price\").isNotNull()\n",
        ").withColumn(\"price_double\", col(\"price\").try_cast(\"double\"))\n",
        "\n",
        "# Filter out any rows where price casting failed (invalid price values)\n",
        "metadata_with_numeric = metadata_with_numeric.filter(col(\"price_double\").isNotNull())\n",
        "\n",
        "total_with_all_numeric = metadata_with_numeric.count()\n",
        "total_metadata = metadata_df.count()\n",
        "print(f\"Products with all numeric fields: {total_with_all_numeric:,} / {total_metadata:,} ({(total_with_all_numeric/total_metadata*100):.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Calculate pairwise correlations (using the cast price column)\n",
        "correlations = metadata_with_numeric.select(\n",
        "    corr(\"average_rating\", \"rating_number\").alias(\"avg_rating_vs_rating_number\"),\n",
        "    corr(\"average_rating\", \"price_double\").alias(\"avg_rating_vs_price\"),\n",
        "    corr(\"rating_number\", \"price_double\").alias(\"rating_number_vs_price\")\n",
        ").collect()[0]\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "print(\"                  | Rating Number | Price\")\n",
        "print(\"------------------|---------------|--------\")\n",
        "print(f\"Average Rating    | {correlations['avg_rating_vs_rating_number']:>13.4f} | {correlations['avg_rating_vs_price']:>6.4f}\")\n",
        "print(f\"Rating Number     | {1.0:>13.4f} | {correlations['rating_number_vs_price']:>6.4f}\")\n",
        "print(f\"Price             | {correlations['rating_number_vs_price']:>13.4f} | {1.0:>6.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Data Cleaning Insights:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"â€¢ Average Rating vs Rating Number: {correlations['avg_rating_vs_rating_number']:.4f}\")\n",
        "print(\"  â†’ Low correlation suggests ratings are independent of review volume\")\n",
        "print(f\"â€¢ Average Rating vs Price: {correlations['avg_rating_vs_price']:.4f}\")\n",
        "print(\"  â†’ Low correlation suggests price doesn't strongly predict rating\")\n",
        "print(f\"â€¢ Rating Number vs Price: {correlations['rating_number_vs_price']:.4f}\")\n",
        "print(\"  â†’ Indicates relationship between product popularity and price\")\n",
        "print(f\"\\nðŸ’¡ Cleaning Decision: All correlations are reasonable. No data quality issues detected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average rating is almost independent of price and number of reviews.\n",
        "\n",
        "The conclusions of the previous analysis on price_range are confirmed: more expensive products receive slightly better ratings, but the overall dependence is weak.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONCLUSIONS\n",
        "\n",
        "### Data quality and completeness\n",
        "\n",
        "1. There are many missing or empty values â€‹â€‹in metadata_df.\n",
        "\n",
        "2. There are no missing values â€‹â€‹in reviews_df, the data is more structured and complete.\n",
        "\n",
        "### Basic statistics on goods\n",
        "\n",
        "1. Average product rating varies by category.\n",
        "\n",
        "2. Price ranges vary significantly.\n",
        "\n",
        "3. main_category â€” Industrial and Scientific\n",
        "\n",
        "4. Data covers a wide range of prices and ratings.\n",
        "\n",
        "### Reviews analysis\n",
        "\n",
        "1. Number of reviews varies by category.\n",
        "\n",
        "2. Most reviews are 5 stars, many are positive, fewer are negative (1â€“2 stars).\n",
        "\n",
        "3. Verified purchase: ~90% confirmed by purchase â†’ most reviews are reliable.\n",
        "\n",
        "4. Text length: average varies, maximum can be very long â†’ high variability, there are short and extremely long reviews.\n",
        "\n",
        "5. Repeated entries by the same user for the same product at the same time may be detected â†’ duplicate entries need to be cleaned.\n",
        "\n",
        "6. Reviews are generally reliable, positive, but there are anomalies (repeated entries, empty texts) that should be removed before in-depth analysis.\n",
        "\n",
        "### Price analysis\n",
        "\n",
        "1. More expensive products may receive slightly higher ratings, but the overall relationship is weak.\n",
        "\n",
        "2. The bulk of products vary by category.\n",
        "\n",
        "3. The correlation between price and rating is typically low â†’ price is not a strong factor for evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA PREPARATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DATA CLEANING - REVIEWS\n",
            "======================================================================\n",
            "\n",
            "1. Checking for duplicates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original count: 5,183,005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate groups found: 45,098\n",
            "\n",
            "Top 10 duplicate patterns:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+----------+-------------+-----+\n",
            "|user_id                     |asin      |timestamp    |count|\n",
            "+----------------------------+----------+-------------+-----+\n",
            "|AGALPU5ARZEK75CGKYELK232AHGA|B07SYB2BFW|1616840817793|27   |\n",
            "|AHK2K4QJG2LOFDO24VTVMLH3SDKA|B07S8K4F5J|1642106258544|11   |\n",
            "|AHJETSJDQNQDIRL66CV5LO26UNUQ|B01LXYM03A|1551424666757|10   |\n",
            "|AG7QXEUHBSFEUUW46T7ECEDJSNHQ|B01C5QR4HS|1490123759000|10   |\n",
            "|AFFKTOSWUZCSSHHMTMYPMQJTUAKQ|B076KNYCZ6|1626279666858|10   |\n",
            "|AHYVEMREIHVJYUXFD5HPAJ5FWMLA|B0BLYW7YKH|1693071873887|9    |\n",
            "|AEP3ESK7PUBKZGHW3HZKAWHQWW5A|B01AGQ3RVQ|1617291783217|9    |\n",
            "|AGRTAIXLOKGMMKT77XPBHUQ3Y33A|B00BUF0YLO|1603564758066|9    |\n",
            "|AFI6SGYRSOTESKN26JO53BGBZB6A|B0B1HVR5LL|1670853016471|9    |\n",
            "|AEAP7AJTQY65MVRHKPP2MMUYDU6Q|B07MQBQ7Y5|1681771589179|9    |\n",
            "+----------------------------+----------+-------------+-----+\n",
            "only showing top 10 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 148:==========================================>            (10 + 3) / 13]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total duplicate records: 98,090\n",
            "Impact: 1.89% of data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"DATA CLEANING - REVIEWS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1. Checking for duplicates...\")\n",
        "original_review_count = reviews_df.count()\n",
        "print(f\"Original count: {original_review_count:,}\")\n",
        "\n",
        "duplicates = reviews_df.groupBy(\"user_id\", \"asin\", \"timestamp\") \\\n",
        "    .count() \\\n",
        "    .filter(col(\"count\") > 1) \\\n",
        "    .orderBy(col(\"count\").desc())\n",
        "\n",
        "duplicate_groups = duplicates.count()\n",
        "print(f\"Duplicate groups found: {duplicate_groups:,}\")\n",
        "\n",
        "if duplicate_groups > 0:\n",
        "    # Show top duplicate patterns\n",
        "    print(\"\\nTop 10 duplicate patterns:\")\n",
        "    duplicates.show(10, truncate=False)\n",
        "    \n",
        "    # Calculate impact\n",
        "    total_duplicate_records = duplicates.agg(spark_sum(\"count\")).collect()[0][\"sum(count)\"]\n",
        "    print(f\"Total duplicate records: {total_duplicate_records:,}\")\n",
        "    print(f\"Impact: {(total_duplicate_records/original_review_count*100):.2f}% of data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. Removing duplicates (keeping first occurrence)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 154:>                                                      (0 + 12) / 13]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After removing duplicates: 5,130,013\n",
            "Removed: 52,992 duplicate records (1.02%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "print(\"\\n2. Removing duplicates (keeping first occurrence)...\")\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\", \"asin\", \"timestamp\") \\\n",
        "                    .orderBy(\"rating\")\n",
        "\n",
        "reviews_cleaned = reviews_df.withColumn(\n",
        "    \"row_num\",\n",
        "    row_number().over(window_spec)\n",
        ").filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
        "\n",
        "cleaned_count = reviews_cleaned.count()\n",
        "removed = original_review_count - cleaned_count\n",
        "print(f\"After removing duplicates: {cleaned_count:,}\")\n",
        "print(f\"Removed: {removed:,} duplicate records ({(removed/original_review_count*100):.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Checking for empty/invalid reviews...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews with empty text: 5,086 (0.10%)\n",
            "\n",
            "Sample empty reviews:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------+---------------------------------------------------------------------------+----+\n",
            "|asin      |rating|title                                                                      |text|\n",
            "+----------+------+---------------------------------------------------------------------------+----+\n",
            "|B07PVVJWHK|1.0   |Used                                                                       |    |\n",
            "|B07TCD93NK|5.0   |Perfect for my classroom!                                                  |    |\n",
            "|B00AWRR662|5.0   |Was vey easy to use and very strong                                        |    |\n",
            "|B094J2B328|5.0   |This alone helped me a lot during my move out. Thanks                      |    |\n",
            "|B09B9F4KVZ|1.0   |This is the 2nd pair of shears that broke from my pack in not even a month.|    |\n",
            "+----------+------+---------------------------------------------------------------------------+----+\n",
            "only showing top 5 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 169:===================================================>   (15 + 1) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After removing empty texts: 5,124,927\n",
            "Removed: 5,086 empty reviews (0.10%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, length, trim\n",
        "\n",
        "print(\"\\n3. Checking for empty/invalid reviews...\")\n",
        "\n",
        "empty_text = reviews_cleaned.filter(\n",
        "    (col(\"text\").isNull()) |\n",
        "    (length(trim(col(\"text\"))) == 0)\n",
        ").count()\n",
        "\n",
        "print(f\"Reviews with empty text: {empty_text:,} ({(empty_text/cleaned_count*100):.2f}%)\")\n",
        "\n",
        "# Show sample empty reviews before removing\n",
        "if empty_text > 0:\n",
        "    print(\"\\nSample empty reviews:\")\n",
        "    reviews_cleaned.filter(\n",
        "        (col(\"text\").isNull()) |\n",
        "        (length(trim(col(\"text\"))) == 0)\n",
        "    ).select(\"asin\", \"rating\", \"title\", \"text\").show(5, truncate=False)\n",
        "\n",
        "reviews_cleaned = reviews_cleaned.filter(\n",
        "    (col(\"text\").isNotNull()) &\n",
        "    (length(trim(col(\"text\"))) > 0)\n",
        ")\n",
        "\n",
        "final_review_count = reviews_cleaned.count()\n",
        "removed_empty = cleaned_count - final_review_count\n",
        "print(f\"\\nAfter removing empty texts: {final_review_count:,}\")\n",
        "print(f\"Removed: {removed_empty:,} empty reviews ({(removed_empty/cleaned_count*100):.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. Checking rating validity...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 175:============================================>          (13 + 3) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid ratings (outside 1-5 range): 0\n",
            "âœ“ All ratings are within valid range (1.0 - 5.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"\\n4. Checking rating validity...\")\n",
        "\n",
        "invalid_ratings = reviews_cleaned.filter(\n",
        "    (col(\"rating\") < 1.0) |\n",
        "    (col(\"rating\") > 5.0)\n",
        ").count()\n",
        "\n",
        "print(f\"Invalid ratings (outside 1-5 range): {invalid_ratings}\")\n",
        "\n",
        "if invalid_ratings > 0:\n",
        "    print(\"\\nâš ï¸  WARNING: Found invalid ratings!\")\n",
        "    reviews_cleaned.filter(\n",
        "        (col(\"rating\") < 1.0) |\n",
        "        (col(\"rating\") > 5.0)\n",
        "    ).select(\"asin\", \"rating\", \"title\").show(10, truncate=False)\n",
        "    print(\"\\nðŸ’¡ Cleaning Decision: These should be removed or corrected.\")\n",
        "else:\n",
        "    print(\"âœ“ All ratings are within valid range (1.0 - 5.0)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DATA CLEANING - METADATA\n",
            "======================================================================\n",
            "\n",
            "1. Checking metadata duplicates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original count: 427,564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 182:>                                                      (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After removing duplicates: 427,564\n",
            "Removed: 0 duplicate products (0.00%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATA CLEANING - METADATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1. Checking metadata duplicates...\")\n",
        "original_meta_count = metadata_df.count()\n",
        "print(f\"Original count: {original_meta_count:,}\")\n",
        "\n",
        "metadata_cleaned = metadata_df.dropDuplicates([\"parent_asin\"])\n",
        "\n",
        "cleaned_meta_count = metadata_cleaned.count()\n",
        "removed_meta = original_meta_count - cleaned_meta_count\n",
        "print(f\"After removing duplicates: {cleaned_meta_count:,}\")\n",
        "print(f\"Removed: {removed_meta:,} duplicate products ({(removed_meta/original_meta_count*100):.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. Checking products without title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Products without title: 25 (0.01%)\n",
            "\n",
            "Sample products without title:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 194:>                                                      (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----+--------------+\n",
            "|parent_asin|title|average_rating|\n",
            "+-----------+-----+--------------+\n",
            "|B087722LS3 |     |3.6           |\n",
            "|B003XEY1DW |     |5.0           |\n",
            "|B01BFJQS64 |     |4.4           |\n",
            "|B0073B6RCY |     |5.0           |\n",
            "|B00A8OQ71E |     |4.1           |\n",
            "+-----------+-----+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "ðŸ’¡ Cleaning Decision: Remove products without titles as they're not useful.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"\\n2. Checking products without title...\")\n",
        "\n",
        "no_title = metadata_cleaned.filter(\n",
        "    (col(\"title\").isNull()) |\n",
        "    (length(trim(col(\"title\"))) == 0)\n",
        ").count()\n",
        "\n",
        "print(f\"Products without title: {no_title} ({(no_title/cleaned_meta_count*100):.2f}%)\")\n",
        "\n",
        "if no_title > 0:\n",
        "    print(\"\\nSample products without title:\")\n",
        "    metadata_cleaned.filter(\n",
        "        (col(\"title\").isNull()) |\n",
        "        (length(trim(col(\"title\"))) == 0)\n",
        "    ).select(\"parent_asin\", \"title\", \"average_rating\").show(5, truncate=False)\n",
        "    print(\"\\nðŸ’¡ Cleaning Decision: Remove products without titles as they're not useful.\")\n",
        "else:\n",
        "    print(\"âœ“ All products have titles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_cleaned = metadata_cleaned.filter(\n",
        "    (col(\"title\").isNotNull()) &\n",
        "    (length(trim(col(\"title\"))) > 0)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Checking price anomalies...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Products with price < $0.10: 6\n",
            "\n",
            "Price Statistics:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------+\n",
            "|summary|        price_num|\n",
            "+-------+-----------------+\n",
            "|  count|           222931|\n",
            "|   mean|60.79625920127254|\n",
            "| stddev|471.5184303124185|\n",
            "|    min|             0.01|\n",
            "|    max|        129266.64|\n",
            "+-------+-----------------+\n",
            "\n",
            "\n",
            "Lowest prices (potential anomalies):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------------------------------------+--------------+\n",
            "|     price|                                             title|average_rating|\n",
            "+----------+--------------------------------------------------+--------------+\n",
            "|         â€”|Pinniped Projects: Articulating Seal and Sea Li...|           5.0|\n",
            "|         â€”|               HUF Mechanics Pullover Hoodie Black|           5.0|\n",
            "|from 29.99|  Understanding Colorectal Cancer Anatomical Chart|           5.0|\n",
            "|         â€”|Anno Womens Bleach Friendly Surgical Cap Scrub ...|           2.9|\n",
            "|         â€”|HobbyKing Compact 20A Watt Meter and Servo Powe...|           5.0|\n",
            "|         â€”|  1/24 ã‚ªãƒ¼ãƒŠãƒ¼ã‚º24 No.12 '73 ãƒ€ãƒƒã‚¸ãƒãƒ£ãƒ¬ãƒ³ã‚¸ãƒ£ãƒ¼|           4.7|\n",
            "|         â€”|Chemistry Equations & Answers Laminate Referenc...|           5.0|\n",
            "| from 9.98|        Handbook, DOT Hazmat Requirements, English|           4.7|\n",
            "|         â€”|ACLS (Advanced Cardiac Life Support) Survival C...|           4.3|\n",
            "|         â€”|La Biblica Ilustrada-OS-Easy-To-Read (Spanish E...|           5.0|\n",
            "+----------+--------------------------------------------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Highest prices:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 214:============================>                           (6 + 6) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------------------------------------------------+--------------+\n",
            "|    price|                                             title|average_rating|\n",
            "+---------+--------------------------------------------------+--------------+\n",
            "|129266.64|Senco 08S250W592 2-1/2\" x #8 Duraspin Collated ...|           3.5|\n",
            "|124024.12|Merit Glue Bond Refill for 350-RP UNSCORED, Alu...|           4.0|\n",
            "|  49999.0|JG MAKER Industrial SLA 3D Printer JG-A600 Larg...|           5.0|\n",
            "| 32999.99|On/Go One COVID-19 Rapid Antigen Home Test, 1 P...|           5.0|\n",
            "| 30769.65|                 Weight Set(20), 50 kg-1 g, ASTM 1|           5.0|\n",
            "| 25666.25|     SPX 2 JAWPULLER, 100 TON, Universal (PH1002J)|           5.0|\n",
            "| 21385.47|GOLEHS Osmium (Os) Density Cube, Laboratory-Gra...|           1.0|\n",
            "| 15334.52|Starrett 123Z-72 Vernier Caliper, Steel, Nib St...|           3.0|\n",
            "| 15183.99|OTC 1854 100-Ton Capacity Shop Press with Elect...|           5.0|\n",
            "| 14530.99|Fluke Networks DSX2-8000 CableAnalyzer Copper C...|           4.6|\n",
            "+---------+--------------------------------------------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "ðŸ’¡ Cleaning Decision: Review extreme prices manually. Very low prices (<$0.10) may be data errors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"\\n3. Checking price anomalies...\")\n",
        "\n",
        "# Check for very low prices (potential data quality issues)\n",
        "very_low_prices = metadata_cleaned.filter(\n",
        "    col(\"price\").isNotNull() & \n",
        "    (col(\"price\").try_cast(\"double\") < 0.1)\n",
        ").count()\n",
        "\n",
        "print(f\"Products with price < $0.10: {very_low_prices}\")\n",
        "\n",
        "# Show price distribution\n",
        "price_stats = metadata_cleaned.filter(col(\"price\").isNotNull()) \\\n",
        "    .select(\n",
        "        col(\"price\").try_cast(\"double\").alias(\"price_num\")\n",
        "    ) \\\n",
        "    .describe()\n",
        "\n",
        "print(\"\\nPrice Statistics:\")\n",
        "price_stats.show()\n",
        "\n",
        "# Show lowest and highest prices\n",
        "print(\"\\nLowest prices (potential anomalies):\")\n",
        "metadata_cleaned.filter(col(\"price\").isNotNull()) \\\n",
        "    .select(\"price\", \"title\", \"average_rating\") \\\n",
        "    .orderBy(col(\"price\").try_cast(\"double\")) \\\n",
        "    .show(10, truncate=50)\n",
        "\n",
        "print(\"\\nHighest prices:\")\n",
        "metadata_cleaned.filter(col(\"price\").isNotNull()) \\\n",
        "    .select(\"price\", \"title\", \"average_rating\") \\\n",
        "    .orderBy(col(\"price\").try_cast(\"double\").desc()) \\\n",
        "    .show(10, truncate=50)\n",
        "\n",
        "print(\"\\nðŸ’¡ Cleaning Decision: Review extreme prices manually. Very low prices (<$0.10) may be data errors.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. Checking invalid average ratings in metadata...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid average ratings: 0\n",
            "âœ“ All average ratings are within valid range (0.0 - 5.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 223:>                                                      (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Products with 0 rating_number but non-null average_rating: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"\\n4. Checking invalid average ratings in metadata...\")\n",
        "\n",
        "invalid_avg = metadata_cleaned.filter(\n",
        "    (col(\"average_rating\") < 0) |\n",
        "    (col(\"average_rating\") > 5)\n",
        ").count()\n",
        "\n",
        "print(f\"Invalid average ratings: {invalid_avg}\")\n",
        "\n",
        "if invalid_avg > 0:\n",
        "    print(\"\\nâš ï¸  WARNING: Found invalid average ratings!\")\n",
        "    metadata_cleaned.filter(\n",
        "        (col(\"average_rating\") < 0) |\n",
        "        (col(\"average_rating\") > 5)\n",
        "    ).select(\"parent_asin\", \"title\", \"average_rating\", \"rating_number\").show(10, truncate=False)\n",
        "    print(\"\\nðŸ’¡ Cleaning Decision: These should be removed or corrected.\")\n",
        "else:\n",
        "    print(\"âœ“ All average ratings are within valid range (0.0 - 5.0)\")\n",
        "    \n",
        "# Additional check: products with 0 rating_number but non-null average_rating\n",
        "inconsistent_ratings = metadata_cleaned.filter(\n",
        "    (col(\"rating_number\") == 0) &\n",
        "    col(\"average_rating\").isNotNull()\n",
        ").count()\n",
        "print(f\"\\nProducts with 0 rating_number but non-null average_rating: {inconsistent_ratings}\")\n",
        "if inconsistent_ratings > 0:\n",
        "    print(\"ðŸ’¡ Cleaning Decision: These may need review - rating_number should match average_rating presence.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "SAVING CLEANED DATA\n",
            "======================================================================\n",
            "\n",
            "Saving cleaned reviews to: /Users/andriimyrosh/Projects/amazon-reviews-analysis/data/cleaned/review_categories/industrial_and_scientific_reviews_cleaned.parquet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Reviews saved successfully\n",
            "\n",
            "Saving cleaned metadata to: /Users/andriimyrosh/Projects/amazon-reviews-analysis/data/cleaned/meta_categories/industrial_and_scientific_metadata_cleaned.parquet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 234:==================================================>    (12 + 1) / 13]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Metadata saved successfully\n",
            "\n",
            "======================================================================\n",
            "FINAL CLEANING STATISTICS\n",
            "======================================================================\n",
            "\n",
            "REVIEWS:\n",
            "  Original:        5,183,005\n",
            "  Cleaned:         5,124,927\n",
            "  Removed:            58,078 (1.12%)\n",
            "\n",
            "METADATA:\n",
            "  Original:          427,564\n",
            "  Cleaned:           427,564\n",
            "  Removed:                 0 (0.00%)\n",
            "\n",
            "TOTAL:\n",
            "  Records removed:       58,078\n",
            "  Data retention:         98.96%\n",
            "\n",
            "======================================================================\n",
            "âœ“ Data cleaning completed successfully!\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING CLEANED DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Ensure cleaned directories exist\n",
        "cleaned_reviews_dir = ROOT_DIR / \"data/cleaned/review_categories\"\n",
        "cleaned_meta_dir = ROOT_DIR / \"data/cleaned/meta_categories\"\n",
        "cleaned_reviews_dir.mkdir(parents=True, exist_ok=True)\n",
        "cleaned_meta_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CLEANED_REVIEWS_PATH = cleaned_reviews_dir / \"industrial_and_scientific_reviews_cleaned.parquet\"\n",
        "CLEANED_METADATA_PATH = cleaned_meta_dir / \"industrial_and_scientific_metadata_cleaned.parquet\"\n",
        "\n",
        "print(f\"\\nSaving cleaned reviews to: {CLEANED_REVIEWS_PATH}\")\n",
        "reviews_cleaned.write.mode(\"overwrite\").parquet(str(CLEANED_REVIEWS_PATH))\n",
        "print(\"âœ“ Reviews saved successfully\")\n",
        "\n",
        "print(f\"\\nSaving cleaned metadata to: {CLEANED_METADATA_PATH}\")\n",
        "metadata_cleaned.write.mode(\"overwrite\").parquet(str(CLEANED_METADATA_PATH))\n",
        "print(\"âœ“ Metadata saved successfully\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL CLEANING STATISTICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "reviews_removed = original_review_count - final_review_count\n",
        "meta_removed = original_meta_count - cleaned_meta_count\n",
        "total_removed = reviews_removed + meta_removed\n",
        "\n",
        "print(f\"\\nREVIEWS:\")\n",
        "print(f\"  Original:     {original_review_count:>12,}\")\n",
        "print(f\"  Cleaned:      {final_review_count:>12,}\")\n",
        "print(f\"  Removed:      {reviews_removed:>12,} ({(reviews_removed/original_review_count*100):.2f}%)\")\n",
        "\n",
        "print(f\"\\nMETADATA:\")\n",
        "print(f\"  Original:     {original_meta_count:>12,}\")\n",
        "print(f\"  Cleaned:      {cleaned_meta_count:>12,}\")\n",
        "print(f\"  Removed:      {meta_removed:>12,} ({(meta_removed/original_meta_count*100):.2f}%)\")\n",
        "\n",
        "print(f\"\\nTOTAL:\")\n",
        "print(f\"  Records removed: {total_removed:>12,}\")\n",
        "print(f\"  Data retention:   {(100 - (total_removed/(original_review_count + original_meta_count)*100)):>11.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"âœ“ Data cleaning completed successfully!\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
