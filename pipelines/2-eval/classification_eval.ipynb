{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef3162c",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(Path.cwd(), indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "ROOT_DIR = Path(os.environ.get(\"PROJECT_ROOT\", Path.cwd()))\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8e81e",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.amazon_reviews_analysis.utils import build_spark\n",
    "\n",
    "spark = build_spark()\n",
    "\n",
    "print(\"✓ Spark Session created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630ac48",
   "metadata": {},
   "source": [
    "## 3. Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "MODEL_DIR = ROOT_DIR / \"models\" / \"spark_lr_classifier\"\n",
    "\n",
    "model = PipelineModel.load(str(MODEL_DIR))\n",
    "\n",
    "print(f\"✓ Model loaded from {MODEL_DIR}\")\n",
    "print(f\"Pipeline stages: {[stage.__class__.__name__ for stage in model.stages]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab15cf",
   "metadata": {},
   "source": [
    "## 4. Load Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b86f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data/classification/extracted\"\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "TARGET_COL = \"label\"\n",
    "\n",
    "df = spark.read.parquet(str(DATA_DIR))\n",
    "df = df.withColumn(\"label\", col(TARGET_COL).cast(\"double\"))\n",
    "\n",
    "# Use same split as training (seed=42) to get the same test set\n",
    "_, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Test set: {test_df.count():,} records\")\n",
    "test_df.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16cd5f1",
   "metadata": {},
   "source": [
    "## 5. Make Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "predictions.select(TEXT_COL, \"label\", \"prediction\", \"probability\").show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a7bf0",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "metrics = {}\n",
    "for metric_name in [\"accuracy\", \"f1\", \"weightedPrecision\", \"weightedRecall\"]:\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=metric_name)\n",
    "    metrics[metric_name] = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RESULTS\")\n",
    "print(f\"Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score:           {metrics['f1']:.4f}\")\n",
    "print(f\"Weighted Precision: {metrics['weightedPrecision']:.4f}\")\n",
    "print(f\"Weighted Recall:    {metrics['weightedRecall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b90f8b",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac78bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix (rows=actual, cols=predicted):\")\n",
    "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "confusion_matrix.show(25)\n",
    "\n",
    "pivot_cm = predictions.groupBy(\"label\").pivot(\"prediction\").count().orderBy(\"label\").fillna(0)\n",
    "print(\"\\nConfusion Matrix (pivoted):\")\n",
    "pivot_cm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a673d",
   "metadata": {},
   "source": [
    "## 8. Per-Class Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum as spark_sum, when\n",
    "\n",
    "LABEL_NAMES = {0.0: \"negative\", 1.0: \"neutral\", 2.0: \"positive\"}\n",
    "\n",
    "per_class = predictions.groupBy(\"label\").agg(\n",
    "    count(\"*\").alias(\"total\"), spark_sum(when(col(\"label\") == col(\"prediction\"), 1).otherwise(0)).alias(\"correct\")\n",
    ")\n",
    "per_class = per_class.withColumn(\"accuracy\", col(\"correct\") / col(\"total\"))\n",
    "\n",
    "print(\"Per-Class Accuracy:\")\n",
    "for row in per_class.orderBy(\"label\").collect():\n",
    "    label_name = LABEL_NAMES.get(row[\"label\"], str(row[\"label\"]))\n",
    "    print(f\"{label_name:10s}: {row['accuracy']:.4f} ({row['correct']}/{row['total']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2081e98",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f828632",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = predictions.filter(col(\"label\") != col(\"prediction\"))\n",
    "\n",
    "print(f\"Total misclassified: {misclassified.count():,}\")\n",
    "print(f\"Error rate: {misclassified.count() / predictions.count() * 100:.2f}%\")\n",
    "print(\"\\nSample misclassified reviews:\")\n",
    "misclassified.select(TEXT_COL, \"label\", \"prediction\").show(10, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
