{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c544c4f",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca03b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(Path.cwd(), indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "ROOT_DIR = Path(os.environ.get(\"PROJECT_ROOT\", Path.cwd()))\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092250d1",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from src.amazon_reviews_analysis.utils import build_spark\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# CRITICAL: Set JAVA_HOME BEFORE importing Spark\n",
    "# This must be done in the notebook, not just in terminal\n",
    "\n",
    "# Try multiple methods to find Java\n",
    "java_home = None\n",
    "\n",
    "# Method 1: Check if already set\n",
    "if os.environ.get('JAVA_HOME'):\n",
    "    java_home = os.environ['JAVA_HOME']\n",
    "    print(f\"‚úì JAVA_HOME already set to: {java_home}\")\n",
    "else:\n",
    "    # Method 2: Try Homebrew\n",
    "    try:\n",
    "        brew_prefix = subprocess.check_output(\n",
    "            ['brew', '--prefix', 'openjdk@17'], \n",
    "            text=True,\n",
    "            stderr=subprocess.DEVNULL\n",
    "        ).strip()\n",
    "        if os.path.exists(brew_prefix):\n",
    "            java_home = brew_prefix\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            print(f\"‚úì Found Java via Homebrew: {java_home}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 3: Try common locations\n",
    "    if not java_home:\n",
    "        common_paths = [\n",
    "            '/opt/homebrew/opt/openjdk@17',\n",
    "            '/usr/local/opt/openjdk@17',\n",
    "            '/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home',\n",
    "            '/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home'\n",
    "        ]\n",
    "        for path in common_paths:\n",
    "            if os.path.exists(path):\n",
    "                java_home = path\n",
    "                os.environ['JAVA_HOME'] = java_home\n",
    "                print(f\"‚úì Found Java at: {java_home}\")\n",
    "                break\n",
    "    \n",
    "    # Method 4: Use /usr/libexec/java_home (macOS)\n",
    "    if not java_home:\n",
    "        try:\n",
    "            java_home = subprocess.check_output(\n",
    "                ['/usr/libexec/java_home', '-v', '17'],\n",
    "                text=True,\n",
    "                stderr=subprocess.DEVNULL\n",
    "            ).strip()\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            print(f\"‚úì Found Java via java_home: {java_home}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Verify Java is accessible\n",
    "if java_home:\n",
    "    java_bin = os.path.join(java_home, 'bin', 'java')\n",
    "    if os.path.exists(java_bin):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [java_bin, '-version'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                timeout=5\n",
    "            )\n",
    "            print(f\"‚úì Java verification successful\")\n",
    "            print(f\"  Version: {result.stdout.split(chr(10))[0] if result.stdout else 'N/A'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not verify Java: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Java binary not found at: {java_bin}\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: Could not find Java installation!\")\n",
    "    print(\"Please install Java 17: brew install openjdk@17\")\n",
    "    print(\"Or set JAVA_HOME manually in this cell:\")\n",
    "    print(\"  os.environ['JAVA_HOME'] = '/path/to/java'\")\n",
    "\n",
    "# Also add to PATH\n",
    "if java_home:\n",
    "    java_bin_dir = os.path.join(java_home, 'bin')\n",
    "    current_path = os.environ.get('PATH', '')\n",
    "    if java_bin_dir not in current_path:\n",
    "        os.environ['PATH'] = f\"{java_bin_dir}:{current_path}\"\n",
    "        print(f\"‚úì Added Java to PATH\")\n",
    "# Initialize Spark\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Recreate Spark session\n",
    "from src.amazon_reviews_analysis.utils import build_spark\n",
    "spark = build_spark()\n",
    "print(\"‚úì Spark Session created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e17531",
   "metadata": {},
   "source": [
    "## 3. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49279ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ZIP = ROOT_DIR / \"data/classification/classification_reviews.zip\"\n",
    "EXTRACT_DIR = ROOT_DIR / \"data/classification/extracted\"\n",
    "\n",
    "# Check if zip file exists and is valid\n",
    "if not DATA_ZIP.exists():\n",
    "    raise FileNotFoundError(f\"Zip file not found: {DATA_ZIP}\")\n",
    "\n",
    "# Check file size (empty files are likely corrupted)\n",
    "if DATA_ZIP.stat().st_size == 0:\n",
    "    raise ValueError(f\"Zip file is empty: {DATA_ZIP}\")\n",
    "\n",
    "# Try to verify it's a valid zip file\n",
    "try:\n",
    "    with zipfile.ZipFile(DATA_ZIP, \"r\") as test_zip:\n",
    "        test_zip.testzip()  # Test the zip file integrity\n",
    "except zipfile.BadZipFile:\n",
    "    raise ValueError(f\"File is not a valid zip file: {DATA_ZIP}. It may be corrupted or in a different format.\")\n",
    "\n",
    "if not EXTRACT_DIR.exists():\n",
    "    print(f\"üì¶ Extracting {DATA_ZIP}...\")\n",
    "    with zipfile.ZipFile(DATA_ZIP, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(\"‚úì Extraction complete!\")\n",
    "else:\n",
    "    print(\"‚úì Data already extracted\")\n",
    "\n",
    "print(f\"\\nData location: {EXTRACT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(str(EXTRACT_DIR))\n",
    "\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"\\nColumns: {df.columns}\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e524280",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19211038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution (label: 0=negative, 1=neutral, 2=positive)\n",
    "df.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a51c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "TARGET_COL = \"label\"  # 0=negative, 1=neutral, 2=positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045b263",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe31d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Label is already 0, 1, 2 - just cast to double for MLlib\n",
    "df_clean = df.withColumn(\"label\", col(TARGET_COL).cast(\"double\"))\n",
    "\n",
    "print(f\"Clean dataset: {df_clean.count():,} records\")\n",
    "print(\"\\nLabel distribution (0=negative, 1=neutral, 2=positive):\")\n",
    "df_clean.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_df.count():,} records\")\n",
    "print(f\"Test set: {test_df.count():,} records\")\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Get class counts in training set\n",
    "class_counts = train_df.groupBy(\"label\").count().collect()\n",
    "total_samples = train_df.count()\n",
    "\n",
    "# Calculate weights: total_samples / (num_classes * class_count)\n",
    "class_weights = {}\n",
    "for row in class_counts:\n",
    "    label = row[\"label\"]\n",
    "    count = row[\"count\"]\n",
    "    weight = total_samples / (len(class_counts) * count)\n",
    "    class_weights[label] = weight\n",
    "\n",
    "print(f\"\\nClass weights: {class_weights}\")\n",
    "\n",
    "# Add weight column to training data\n",
    "train_df = train_df.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"label\") == 0.0, lit(class_weights.get(0.0, 1.0)))\n",
    "    .when(col(\"label\") == 1.0, lit(class_weights.get(1.0, 1.0)))\n",
    "    .when(col(\"label\") == 2.0, lit(class_weights.get(2.0, 1.0)))\n",
    "    .otherwise(lit(1.0))\n",
    ")\n",
    "\n",
    "print(\"‚úì Class weights added to training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d5078",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=TEXT_COL, outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=5000)  # Reduced from 10000 to save memory\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "print(\"‚úì Feature transformers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbd669",
   "metadata": {},
   "source": [
    "## 7. Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    weightCol=\"weight\",  # Use class weights to handle imbalanced data\n",
    "    numTrees=10,  # Reduced from 10 to save memory (fewer trees = less memory)\n",
    "    maxDepth=3,  # Reduced from 3 to save memory (shallower trees = less memory)\n",
    "    maxBins=16,  # Reduced from 32 to save memory (fewer bins = less memory)\n",
    "    minInstancesPerNode=5,  # Increased from 1 to reduce tree size (fewer nodes = less memory)\n",
    "    minInfoGain=0.0,  # Minimum information gain for a split\n",
    "    impurity=\"gini\",  # Impurity measure: \"gini\" or \"entropy\"\n",
    "    featureSubsetStrategy=\"sqrt\",  # Use sqrt of features per tree (less memory than \"auto\")\n",
    "    subsamplingRate=0.4,  # Use 80% of data per tree (less memory per tree)\n",
    "    seed=42,\n",
    "    \n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training model...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d32fa7",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(TEXT_COL, \"label\", \"prediction\", \"probability\").show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "print(\"RESULTS\")\n",
    "print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "print(f\"F1 Score:           {f1:.4f}\")\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Recall:    {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, when\n",
    "\n",
    "per_class = predictions.groupBy(\"label\").agg(\n",
    "    count(\"*\").alias(\"total\"), spark_sum(when(col(\"label\") == col(\"prediction\"), 1).otherwise(0)).alias(\"correct\")\n",
    ")\n",
    "per_class = per_class.withColumn(\"accuracy\", col(\"correct\") / col(\"total\"))\n",
    "per_class.orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ed742",
   "metadata": {},
   "source": [
    "## 9. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = ROOT_DIR / \"models\" / \"spark_lr_classifier\"\n",
    "\n",
    "model.write().overwrite().save(str(MODEL_DIR))\n",
    "\n",
    "print(f\"‚úì Model saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03a8e5",
   "metadata": {},
   "source": [
    "## 10. Quick Inference Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(str(MODEL_DIR))\n",
    "\n",
    "sample_data = spark.createDataFrame(\n",
    "    [\n",
    "        (\"This product is amazing! Best purchase I've ever made.\",),\n",
    "        (\"Terrible quality, broke after one day. Don't buy!\",),\n",
    "        (\"It's okay, nothing special but does the job.\",),\n",
    "    ],\n",
    "    [TEXT_COL],\n",
    ")\n",
    "\n",
    "sample_predictions = loaded_model.transform(sample_data)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "sample_predictions.select(TEXT_COL, \"prediction\").show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
