{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c544c4f",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ca03b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:22.073788Z",
     "start_time": "2025-12-03T09:55:22.068874Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import rootutils\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = str(Path.home() / \"spark-tmp\")\n",
    "rootutils.setup_root(Path.cwd(), indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "ROOT_DIR = Path(os.environ.get(\"PROJECT_ROOT\", Path.cwd()))\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/olehyaiechnyk/PycharmProjects/amazon-reviews-analysis\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:22.126723Z",
     "start_time": "2025-12-03T09:55:22.087985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 8g --conf spark.executor.memory=8g pyspark-shell\"\n",
    "\n",
    "JAVA17 = \"/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA17\n",
    "os.environ[\"PATH\"] = f\"{JAVA17}/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "import subprocess\n",
    "print(subprocess.run([\"java\", \"-version\"], capture_output=True, text=True).stderr)"
   ],
   "id": "f52a4f2d7add6107",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.15\" 2025-04-15\n",
      "OpenJDK Runtime Environment Temurin-17.0.15+6 (build 17.0.15+6)\n",
      "OpenJDK 64-Bit Server VM Temurin-17.0.15+6 (build 17.0.15+6, mixed mode, sharing)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "092250d1",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6be7376c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:22.147351Z",
     "start_time": "2025-12-03T09:55:22.141689Z"
    }
   },
   "source": [
    "from src.amazon_reviews_analysis.utils import build_spark\n",
    "\n",
    "spark = build_spark()\n",
    "\n",
    "print(\"âœ“ Spark Session created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark Session created successfully!\n",
      "Spark Version: 4.0.1\n",
      "Spark App Name: AmazonReviews\n",
      "Spark Master: local[*]\n",
      "Spark UI: http://172.20.10.5:4041\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "58e17531",
   "metadata": {},
   "source": [
    "## 3. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "49279ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:22.172456Z",
     "start_time": "2025-12-03T09:55:22.169698Z"
    }
   },
   "source": [
    "DATA_ZIP = ROOT_DIR / \"data/classification_reviews.zip\"\n",
    "EXTRACT_DIR = ROOT_DIR / \"data/classification\"\n",
    "\n",
    "if not EXTRACT_DIR.exists():\n",
    "    print(f\"ðŸ“¦ Extracting {DATA_ZIP}...\")\n",
    "    with zipfile.ZipFile(DATA_ZIP, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(\"âœ“ Extraction complete!\")\n",
    "else:\n",
    "    print(\"âœ“ Data already extracted\")\n",
    "\n",
    "print(f\"\\nData location: {EXTRACT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data already extracted\n",
      "\n",
      "Data location: /Users/olehyaiechnyk/PycharmProjects/amazon-reviews-analysis/data/classification\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "92dab569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:22.474113Z",
     "start_time": "2025-12-03T09:55:22.247399Z"
    }
   },
   "source": [
    "df = spark.read.parquet(str(EXTRACT_DIR))\n",
    "\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"\\nColumns: {df.columns}\")\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 35,202,489\n",
      "\n",
      "Columns: ['rating', 'title', 'text', 'verified_purchase', 'parent_asin', 'category_label', 'label']\n",
      "root\n",
      " |-- rating: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- category_label: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "8697e146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:27.204221Z",
     "start_time": "2025-12-03T09:55:26.467381Z"
    }
   },
   "source": [
    "df.show(5, truncate=50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------+--------------------------------------------------+-----------------+-----------+--------------+-----+\n",
      "|rating|                          title|                                              text|verified_purchase|parent_asin|category_label|label|\n",
      "+------+-------------------------------+--------------------------------------------------+-----------------+-----------+--------------+-----+\n",
      "|   5.0|  Perfect for my granddaughters|               Just what my granddaughters wanted.|             true| B0771XZ99Y|        sports|    2|\n",
      "|   3.0|             Pretty but Fragile|It makes an amusing popping sound but I thought...|             true| B0814BFFJH|        sports|    1|\n",
      "|   5.0|                    Love these!|I had been searching for a while for some comfo...|             true| B08DXCXYK9|        sports|    2|\n",
      "|   5.0|                     Five Stars|           They worked just as described. Thanks !|             true| B002QG1WJY|        sports|    2|\n",
      "|   1.0|It does not work as advertised.|Don't waste your money. Very poor quality and i...|            false| B00PB78RT8|        sports|    0|\n",
      "+------+-------------------------------+--------------------------------------------------+-----------------+-----------+--------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "8e524280",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "19211038",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:28.504295Z",
     "start_time": "2025-12-03T09:55:27.212294Z"
    }
   },
   "source": [
    "# Check target distribution (label: 0=negative, 1=neutral, 2=positive)\n",
    "df.groupBy(\"label\").count().orderBy(\"label\").show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==========================================>             (47 + 8) / 62]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|   count|\n",
      "+-----+--------+\n",
      "|    0| 5372399|\n",
      "|    1| 2451737|\n",
      "|    2|27378353|\n",
      "+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "10a51c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:28.525624Z",
     "start_time": "2025-12-03T09:55:28.523844Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "TARGET_COL = \"label\"  # 0=negative, 1=neutral, 2=positive"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "5045b263",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fe31d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:55:30.102529Z",
     "start_time": "2025-12-03T09:55:28.576273Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Label is already 0, 1, 2 - just cast to double for MLlib\n",
    "df_clean = df.withColumn(\"label\", col(TARGET_COL).cast(\"double\"))\n",
    "\n",
    "print(f\"Clean dataset: {df_clean.count():,} records\")\n",
    "print(\"\\nLabel distribution (0=negative, 1=neutral, 2=positive):\")\n",
    "df_clean.groupBy(\"label\").count().orderBy(\"label\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean dataset: 35,202,489 records\n",
      "\n",
      "Label distribution (0=negative, 1=neutral, 2=positive):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================================>   (58 + 4) / 62]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|   count|\n",
      "+-----+--------+\n",
      "|  0.0| 5372399|\n",
      "|  1.0| 2451737|\n",
      "|  2.0|27378353|\n",
      "+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "075b06a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:56:28.742343Z",
     "start_time": "2025-12-03T09:55:30.120912Z"
    }
   },
   "source": [
    "# Train-Test Split\n",
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_df.count():,} records\")\n",
    "print(f\"Test set: {test_df.count():,} records\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 28,158,683 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=======================================================>(61 + 1) / 62]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 7,043,806 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "957d5078",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1093bf1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:56:29.357004Z",
     "start_time": "2025-12-03T09:56:28.867507Z"
    }
   },
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=TEXT_COL, outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "print(\"âœ“ Feature transformers defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Feature transformers defined\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "dedbd669",
   "metadata": {},
   "source": [
    "## 7. Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "749d568b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:56:29.400885Z",
     "start_time": "2025-12-03T09:56:29.367922Z"
    }
   },
   "source": [
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "\n",
    "svm = LinearSVC(\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    ")\n",
    "\n",
    "ovr = OneVsRest(classifier=svm)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, ovr])\n",
    "\n",
    "print(f\"Stages: {[stage.__class__.__name__ for stage in pipeline.getStages()]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stages: ['Tokenizer', 'StopWordsRemover', 'HashingTF', 'IDF', 'OneVsRest']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "c570198d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:06:24.103034Z",
     "start_time": "2025-12-03T09:56:29.436622Z"
    }
   },
   "source": [
    "train_df = train_df.sample(False, 0.3, seed=42)  # 30% of training data\n",
    "\n",
    "print(\"ðŸš€ Training model...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"âœ“ Training complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 12:01:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "46d32fa7",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "723b005c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:06:27.957899Z",
     "start_time": "2025-12-03T10:06:24.341224Z"
    }
   },
   "source": [
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(TEXT_COL, \"label\", \"prediction\").show(10, truncate=50)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1606:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+----------+\n",
      "|                                              text|label|prediction|\n",
      "+--------------------------------------------------+-----+----------+\n",
      "|DO NOT BUY!!!! I thought I was buying just a se...|  0.0|       0.0|\n",
      "|Not quite two years since purchasing this tread...|  0.0|       0.0|\n",
      "|Product did not preform as advertised.<br />The...|  0.0|       2.0|\n",
      "|Ordered this DONUT BEACH TOWEL..... Received a ...|  0.0|       0.0|\n",
      "|I dislike that socks are expensive and there is...|  0.0|       0.0|\n",
      "|Horrible!  Somehow, their definition of &#34;as...|  0.0|       0.0|\n",
      "|We recently went on a two-week vacation camping...|  0.0|       0.0|\n",
      "|Had to review it low. Box showed up, after very...|  0.0|       0.0|\n",
      "|                      $17 over MSRP?!!! Seriously?|  0.0|       2.0|\n",
      "|When you pay over $20 for a very simple 3\" clea...|  0.0|       2.0|\n",
      "+--------------------------------------------------+-----+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "4e0c4250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:18:02.708772Z",
     "start_time": "2025-12-03T10:06:27.982698Z"
    }
   },
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    ")\n",
    "\n",
    "accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "f1       = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "recall    = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "\n",
    "print(\"RESULTS\")\n",
    "print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "print(f\"F1 Score:           {f1:.4f}\")\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Recall:    {recall:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1613:=====================================================>(61 + 1) / 62]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "Accuracy:           0.8299\n",
      "F1 Score:           0.7851\n",
      "Weighted Precision: 0.7905\n",
      "Weighted Recall:    0.8299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "a909126a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:20:59.473994Z",
     "start_time": "2025-12-03T10:18:03.391447Z"
    }
   },
   "source": [
    "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix.show(25)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1615:=====================================================>(61 + 1) / 62]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|  0.0|       0.0| 435482|\n",
      "|  0.0|       1.0|    433|\n",
      "|  0.0|       2.0| 639963|\n",
      "|  1.0|       0.0|  71746|\n",
      "|  1.0|       1.0|    840|\n",
      "|  1.0|       2.0| 418308|\n",
      "|  2.0|       0.0|  66581|\n",
      "|  2.0|       1.0|   1139|\n",
      "|  2.0|       2.0|5409314|\n",
      "+-----+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "ad3e5c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:24:03.358557Z",
     "start_time": "2025-12-03T10:21:00.253741Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, when\n",
    "\n",
    "per_class = predictions.groupBy(\"label\").agg(\n",
    "    count(\"*\").alias(\"total\"), spark_sum(when(col(\"label\") == col(\"prediction\"), 1).otherwise(0)).alias(\"correct\")\n",
    ")\n",
    "per_class = per_class.withColumn(\"accuracy\", col(\"correct\") / col(\"total\"))\n",
    "per_class.orderBy(\"label\").show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+--------------------+\n",
      "|label|  total|correct|            accuracy|\n",
      "+-----+-------+-------+--------------------+\n",
      "|  0.0|1075878| 435482| 0.40476894220348403|\n",
      "|  1.0| 490894|    840|0.001711163713551...|\n",
      "|  2.0|5477034|5409314|  0.9876356436713739|\n",
      "+-----+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "131ed742",
   "metadata": {},
   "source": [
    "## 9. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95be36ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:24:06.567848Z",
     "start_time": "2025-12-03T10:24:03.843601Z"
    }
   },
   "source": [
    "MODEL_DIR = ROOT_DIR / \"models\" / \"spark_svm_classifier\"\n",
    "\n",
    "model.write().overwrite().save(str(MODEL_DIR))\n",
    "\n",
    "print(f\"âœ“ Model saved to {MODEL_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model saved to /Users/olehyaiechnyk/PycharmProjects/amazon-reviews-analysis/models/spark_svm_classifier\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "bd03a8e5",
   "metadata": {},
   "source": [
    "## 10. Quick Inference Test\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3fd0ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:24:08.930937Z",
     "start_time": "2025-12-03T10:24:06.601350Z"
    }
   },
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(str(MODEL_DIR))\n",
    "\n",
    "sample_data = spark.createDataFrame(\n",
    "    [\n",
    "        (\"This product is amazing! Best purchase I've ever made.\",),\n",
    "        (\"Terrible quality, broke after one day. Don't buy!\",),\n",
    "        (\"It's okay, nothing special but does the job.\",),\n",
    "    ],\n",
    "    [TEXT_COL],\n",
    ")\n",
    "\n",
    "sample_predictions = loaded_model.transform(sample_data)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "sample_predictions.select(TEXT_COL, \"prediction\").show(truncate=60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "+------------------------------------------------------+----------+\n",
      "|                                                  text|prediction|\n",
      "+------------------------------------------------------+----------+\n",
      "|This product is amazing! Best purchase I've ever made.|       2.0|\n",
      "|     Terrible quality, broke after one day. Don't buy!|       0.0|\n",
      "|          It's okay, nothing special but does the job.|       2.0|\n",
      "+------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "b891a7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T10:24:09.904725Z",
     "start_time": "2025-12-03T10:24:08.974562Z"
    }
   },
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
