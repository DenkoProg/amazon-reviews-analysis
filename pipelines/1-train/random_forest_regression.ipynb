{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"../../data/cleaned/regression_price.zip\"  \n",
    "extract_path = \"../../data/cleaned/regression_data\"   \n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Unzip...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(extract_path)\n",
    "    print(\"Ready!\")\n",
    "else:\n",
    "    print(\"Folder already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Price Prediction RF\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/cleaned/regression_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 699283\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(DATA_PATH)\n",
    "\n",
    "print(f\"Total rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, length, when\n",
    "\n",
    "has_array_features = [f.dataType for f in df.schema.fields if f.name == \"features\"] # checks the data type of the features column and creates a new column features_count\n",
    "if str(has_array_features[0]).startswith(\"ArrayType\"):\n",
    "    df = df.withColumn(\"features_count\", size(col(\"features\")))\n",
    "else:\n",
    "    if \"features_count\" not in df.columns and \"features\" in df.columns:\n",
    "         df = df.withColumn(\"features_count\", length(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = [\"rating_number\", \"average_rating\", \"main_category\", \"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"features_count\" in df.columns: required_cols.append(\"features_count\")\n",
    "if \"desc_len\" in df.columns: required_cols.append(\"desc_len\")\n",
    "\n",
    "df_clean = df.dropna(subset=required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn(\"title_len\", length(col(\"title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, col, count\n",
    "\n",
    "store_counts = df_clean.groupBy(\"store\").agg(count(\"*\").alias(\"store_freq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_improved = df_clean.join(store_counts, on=\"store\", how=\"left\")\n",
    "df_improved = df_improved.na.fill(0, subset=[\"store_freq\", \"features_count\", \"title_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data and train the model\n",
    "\n",
    "Join additional store statistics, fills missing numeric values, cleans and tokenizes product titles, removes stopwords, converts text into numerical vectors, encodes the product category, assembles all features into a single vector, and finally trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching...: 644239\n",
      "Features: ['category_index', 'store_freq', 'average_rating', 'rating_number', 'title_len', 'features_count', 'title_features']\n",
      "Training with NLP features...\n",
      "Ready!\n",
      "R2 Score: 0.1024\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, StopWordsRemover\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_improved = df_clean.join(broadcast(store_counts), on=\"store\", how=\"left\")\n",
    "\n",
    "cols_to_fill = [\"store_freq\", \"title_len\", \"average_rating\", \"rating_number\"]\n",
    "if \"features_count\" in df_improved.columns: cols_to_fill.append(\"features_count\")\n",
    "if \"desc_len\" in df_improved.columns: cols_to_fill.append(\"desc_len\")\n",
    "\n",
    "df_improved = df_improved.na.fill(0, subset=cols_to_fill)\n",
    "\n",
    "df_improved = df_improved.filter(col(\"title\").isNotNull())\n",
    "df_improved.cache()\n",
    "print(f\"Caching...: {df_improved.count()}\")\n",
    "stages = []\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "stages.append(tokenizer)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "stages.append(remover)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"title_features\", numFeatures=1000)\n",
    "stages.append(hashingTF)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"main_category\", outputCol=\"category_index\", handleInvalid=\"keep\")\n",
    "stages.append(indexer)\n",
    "\n",
    "numeric_cols = [\"category_index\", \"store_freq\", \"average_rating\", \"rating_number\"]\n",
    "if \"title_len\" in df_improved.columns: numeric_cols.append(\"title_len\")\n",
    "if \"features_count\" in df_improved.columns: numeric_cols.append(\"features_count\")\n",
    "if \"desc_len\" in df_improved.columns: numeric_cols.append(\"desc_len\")\n",
    "\n",
    "assembler_inputs = numeric_cols + [\"title_features\"]\n",
    "print(f\"Features: {assembler_inputs}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_vector\")\n",
    "stages.append(assembler)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features_vector\", \n",
    "    labelCol=\"price\", \n",
    "    numTrees=30, \n",
    "    maxDepth=8, \n",
    "    seed=42,\n",
    "    maxBins=64  \n",
    ")\n",
    "stages.append(rf)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "\n",
    "train_data, test_data = df_improved.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Training with NLP features...\")\n",
    "model = pipeline.fit(train_data)\n",
    "print(\"Ready!\")\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "r2_eval = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(f\"R2 Score: {r2_eval.evaluate(predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ../../models/regression/random_forest_price_v1...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../../models/regression/random_forest_price_v1\"\n",
    "\n",
    "print(f\"Save model to {model_path}...\")\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate descriptive statistics and percentiles for the product price distribution, prints min/median/max values, and computes correlations between price and selected numeric features such as ratings, feature counts, or description length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Price distribution:\n",
      "+-------+------------------+\n",
      "|summary|             price|\n",
      "+-------+------------------+\n",
      "|  count|            644239|\n",
      "|   mean|63.137878977474756|\n",
      "| stddev|183.67803447921332|\n",
      "|    min|              0.01|\n",
      "|    max|          21999.98|\n",
      "+-------+------------------+\n",
      "\n",
      "  Min:    $0.01\n",
      "  5%:     $6.25\n",
      "  25%:    $12.99\n",
      "  Median: $23.99\n",
      "  75%:    $49.99\n",
      "  95%:    $219.99\n",
      "  Max:    $21999.98\n",
      "\n",
      "Correlation:\n",
      "  average_rating       <-> price: 0.0094\n",
      "  rating_number        <-> price: -0.0138\n",
      "  features_count       <-> price: 0.0023\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    log1p, expm1, col, when, length, regexp_extract, \n",
    "    lower, expr, percentile_approx, mean, stddev\n",
    ")\n",
    "\n",
    "print(\"\\nPrice distribution:\")\n",
    "df_improved.select(\"price\").describe().show()\n",
    "\n",
    "price_stats = df_improved.agg(\n",
    "    expr(\"percentile_approx(price, 0.05) as p5\"),\n",
    "    expr(\"percentile_approx(price, 0.25) as p25\"),\n",
    "    expr(\"percentile_approx(price, 0.50) as median\"),\n",
    "    expr(\"percentile_approx(price, 0.75) as p75\"),\n",
    "    expr(\"percentile_approx(price, 0.95) as p95\"),\n",
    "    expr(\"min(price) as min_price\"),\n",
    "    expr(\"max(price) as max_price\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Min:    ${price_stats['min_price']:.2f}\")\n",
    "print(f\"  5%:     ${price_stats['p5']:.2f}\")\n",
    "print(f\"  25%:    ${price_stats['p25']:.2f}\")\n",
    "print(f\"  Median: ${price_stats['median']:.2f}\")\n",
    "print(f\"  75%:    ${price_stats['p75']:.2f}\")\n",
    "print(f\"  95%:    ${price_stats['p95']:.2f}\")\n",
    "print(f\"  Max:    ${price_stats['max_price']:.2f}\")\n",
    "\n",
    "print(\"\\nCorrelation:\")\n",
    "\n",
    "numeric_features = [\"average_rating\", \"rating_number\"]\n",
    "if \"features_count\" in df_improved.columns:\n",
    "    numeric_features.append(\"features_count\")\n",
    "if \"desc_len\" in df_improved.columns:\n",
    "    numeric_features.append(\"desc_len\")\n",
    "\n",
    "for feat in numeric_features:\n",
    "    if feat in df_improved.columns:\n",
    "        corr = df_improved.stat.corr(\"price\", feat)\n",
    "        print(f\"  {feat:20s} <-> price: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from Price Distribution:\n",
    "\n",
    "1. Highly skewed price distribution\n",
    "\n",
    "    The average price is $63, but the median is only $23.99, meaning most products are inexpensive while a small number of very expensive products pull the mean upward.\n",
    "\n",
    "2. Most prices fall in the lower range\n",
    "\n",
    "    25% of products cost $12.99 or less.\n",
    "\n",
    "    75% cost $49.99 or less.\n",
    "\n",
    "    Only 5% exceed $219.99.\n",
    "\n",
    "3. There are extreme outliers\n",
    "\n",
    "    Maximum price is $21,999, which is far above typical product prices and indicates the presence of niche or incorrectly priced items.\n",
    "\n",
    "4. Price range is wide\n",
    "\n",
    "    From $0.01 to $21,999, showing very diverse product types or potentially inconsistent listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from Correlation Analysis:\n",
    "\n",
    "1. All correlations between numerical features and price are close to 0, meaning almost no linear relationship:\n",
    "\n",
    "    Average rating → price (0.0094)\n",
    "\n",
    "    Product rating has no meaningful correlation with price.\n",
    "\n",
    "    High-rated products are not necessarily more expensive.\n",
    "\n",
    "2. Number of ratings → price (-0.0138)\n",
    "\n",
    "    Popular products (many reviews) are not systematically more expensive or cheaper.\n",
    "\n",
    "    Features count → price (0.0023)\n",
    "\n",
    "3. The number of features extracted from the listing shows no statistical relationship with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit ('3.11.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1614c1b0428f9a7744170f6e726f1b37a0202d0ea9f400a7179d4fdaa8b27423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
