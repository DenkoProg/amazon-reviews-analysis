{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GBT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Price Prediction RF\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/cleaned/regression_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 699283\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(DATA_PATH)\n",
    "\n",
    "print(f\"Total rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, length, when\n",
    "\n",
    "has_array_features = [f.dataType for f in df.schema.fields if f.name == \"features\"]\n",
    "if str(has_array_features[0]).startswith(\"ArrayType\"):\n",
    "    df = df.withColumn(\"features_count\", size(col(\"features\")))\n",
    "else:\n",
    "    if \"features_count\" not in df.columns and \"features\" in df.columns:\n",
    "         df = df.withColumn(\"features_count\", length(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = [\"rating_number\", \"average_rating\", \"main_category\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"features_count\" in df.columns: required_cols.append(\"features_count\")\n",
    "if \"desc_len\" in df.columns: required_cols.append(\"desc_len\")\n",
    "\n",
    "df_clean = df.dropna(subset=required_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn(\"title_len\", length(col(\"title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, col, count\n",
    "\n",
    "store_counts = df_clean.groupBy(\"store\").agg(count(\"*\").alias(\"store_freq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_improved = df_clean.join(store_counts, on=\"store\", how=\"left\")\n",
    "df_improved = df_improved.na.fill(0, subset=[\"store_freq\", \"features_count\", \"title_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data and training the model\n",
    "\n",
    "This code transforms the target variable using log1p(price) to stabilize variance and improve training.\n",
    "\n",
    "It prepares NLP and numeric features, builds a full pipeline, and trains a Gradient Boosted Trees model.\n",
    "\n",
    "After training, it converts predictions back to real prices using expm1, evaluates the model using R², RMSE, and MAE, and shows example predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 515254, Test: 128985\n",
      "Training GBT on Log(Price)...\n",
      "Ready!\n",
      "R2 (Log Scale): 0.3104\n",
      "RMSE (Real $): 187.57\n",
      "MAE (Real $):  45.27\n",
      "+-------------------------------------------------------------------------------+------+------------------+\n",
      "|title                                                                          |price |prediction_price  |\n",
      "+-------------------------------------------------------------------------------+------+------------------+\n",
      "|Warhammer 40k Adeptus Mechanicus Codex                                         |22.41 |17.730866095623913|\n",
      "|[ST125] 2 Piece VVT-I DOHC Vinyl Sticker JDM Stickers 2JZ Supra Corrolla SILVER|5.99  |5.794295090694468 |\n",
      "|Analog/Dual Shock Controller - Emerald                                         |57.77 |31.35317626681251 |\n",
      "|Beetle Adventure Racing                                                        |43.98 |29.378666176271825|\n",
      "|Wave Race 64 (Japan)                                                           |180.09|29.892769364260023|\n",
      "+-------------------------------------------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log1p, expm1, col\n",
    "from pyspark.ml.regression import GBTRegressor \n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, StopWordsRemover\n",
    "\n",
    "df_log = df_improved.withColumn(\"label\", log1p(col(\"price\")))\n",
    "\n",
    "stages = []\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "stages.append(tokenizer)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "stages.append(remover)\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"title_features\", numFeatures=1000)\n",
    "stages.append(hashingTF)\n",
    "\n",
    "# Indexer\n",
    "indexer = StringIndexer(inputCol=\"main_category\", outputCol=\"category_index\", handleInvalid=\"keep\")\n",
    "stages.append(indexer)\n",
    "\n",
    "# Assembler\n",
    "numeric_cols = [\"category_index\", \"store_freq\", \"average_rating\", \"rating_number\"]\n",
    "if \"title_len\" in df_log.columns: numeric_cols.append(\"title_len\")\n",
    "if \"features_count\" in df_log.columns: numeric_cols.append(\"features_count\")\n",
    "if \"desc_len\" in df_log.columns: numeric_cols.append(\"desc_len\")\n",
    "\n",
    "input_cols = numeric_cols + [\"title_features\"]\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features_vector\")\n",
    "stages.append(assembler)\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features_vector\", \n",
    "    labelCol=\"label\",    \n",
    "    maxIter=50,           \n",
    "    maxDepth=8,           \n",
    "    stepSize=0.1,         \n",
    "    seed=42,\n",
    "    maxBins=64\n",
    ")\n",
    "stages.append(gbt)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "train_data, test_data = df_log.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_data.count()}, Test: {test_data.count()}\")\n",
    "\n",
    "print(\"Training GBT on Log(Price)...\")\n",
    "model = pipeline.fit(train_data)\n",
    "print(\"Ready!\")\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "predictions = predictions.withColumn(\"prediction_price\", expm1(col(\"prediction\")))\n",
    "\n",
    "r2_eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(f\"R2 (Log Scale): {r2_eval.evaluate(predictions):.4f}\")\n",
    "\n",
    "rmse_eval = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction_price\", metricName=\"rmse\")\n",
    "mae_eval = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction_price\", metricName=\"mae\")\n",
    "\n",
    "print(f\"RMSE (Real $): {rmse_eval.evaluate(predictions):.2f}\")\n",
    "print(f\"MAE (Real $):  {mae_eval.evaluate(predictions):.2f}\")\n",
    "\n",
    "predictions.select(\"title\", \"price\", \"prediction_price\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used log(price) instead of raw price because:\n",
    "\n",
    "1. Log transformation reduces the impact of extreme outliers.\n",
    "\n",
    "2. It makes the distribution more symmetric.\n",
    "\n",
    "3. Many ML models perform better when the target follows a near-normal distribution.\n",
    "\n",
    "4. It stabilizes variance and improves error behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save model to../../models/regression/gbt_price_log_v1...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../../models/regression/gbt_price_log_v1\"\n",
    "\n",
    "print(f\"\\nSave model to{model_path}...\")\n",
    "\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBTRegressor significantly outperformed the Random Forest model because:\n",
    "\n",
    "1. GBT learns iteratively and corrects previous errors.\n",
    "\n",
    "2. It captures complex non-linear relationships better.\n",
    "\n",
    "3. It handles skewed and noisy data more effectively.\n",
    "\n",
    "As a result, GBT provides:\n",
    "\n",
    "1. Higher R² on the log scale\n",
    "\n",
    "2. Lower RMSE and MAE after inverse-transforming predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try another way to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of records: 644,239\n",
      "\n",
      "Removing emissions at a price:\n",
      "  Q1 = $12.99, Q3 = $49.99, IQR = $37.00\n",
      "  Deleted: 78,561\n",
      "Deleted with low ratings: 444,684\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    log1p, expm1, col, when, length, regexp_extract, \n",
    "    lower, expr, percentile_approx, mean, stddev\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, VectorAssembler, StandardScaler, \n",
    "    Tokenizer, StopWordsRemover, HashingTF\n",
    ")\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "initial_count = df_improved.count()\n",
    "print(f\"Initial number of records: {initial_count:,}\")\n",
    "\n",
    "price_stats = df_improved.select(\n",
    "    percentile_approx(\"price\", 0.25).alias(\"q1\"),\n",
    "    percentile_approx(\"price\", 0.75).alias(\"q3\")\n",
    ").collect()[0]\n",
    "\n",
    "q1, q3 = price_stats[\"q1\"], price_stats[\"q3\"]\n",
    "iqr = q3 - q1\n",
    "lower_bound = max(1.0, q1 - 1.5 * iqr)  \n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "print(f\"\\nRemoving emissions at a price:\")\n",
    "print(f\"  Q1 = ${q1:.2f}, Q3 = ${q3:.2f}, IQR = ${iqr:.2f}\")\n",
    "\n",
    "df_clean = df_improved.filter(\n",
    "    (col(\"price\") >= lower_bound) & (col(\"price\") <= upper_bound)\n",
    ")\n",
    "print(f\"  Deleted: {initial_count - df_clean.count():,}\")\n",
    "\n",
    "df_clean = df_clean.filter(col(\"rating_number\") >= 5)\n",
    "print(f\"Deleted with low ratings: {df_clean.count():,}\")\n",
    "\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"average_rating\") >= 1.0) & (col(\"average_rating\") <= 5.0)\n",
    ")\n",
    "\n",
    "\n",
    "df_featured = df_clean\n",
    "\n",
    "df_featured = df_featured.withColumn(\n",
    "    \"rating_popularity\", col(\"average_rating\") * log1p(col(\"rating_number\"))\n",
    ")\n",
    "df_featured = df_featured.withColumn(\n",
    "    \"rating_confidence\", when(col(\"rating_number\") < 10, 0)\n",
    "                         .when(col(\"rating_number\") < 50, 1)\n",
    "                         .when(col(\"rating_number\") < 100, 2)\n",
    "                         .otherwise(3)\n",
    ")\n",
    "\n",
    "if \"title\" in df_featured.columns:\n",
    "    df_featured = df_featured.withColumn(\"title_len\", length(col(\"title\")))\n",
    "    \n",
    "    df_featured = df_featured.withColumn(\n",
    "        \"is_premium\", \n",
    "        when(lower(col(\"title\")).rlike(\"premium|deluxe|professional|pro\"), 1).otherwise(0)\n",
    "    )\n",
    "    df_featured = df_featured.withColumn(\n",
    "        \"is_bundle\", \n",
    "        when(lower(col(\"title\")).rlike(\"bundle|pack|set\"), 1).otherwise(0)\n",
    "    )\n",
    "    df_featured = df_featured.withColumn(\n",
    "        \"has_size\", \n",
    "        when(lower(col(\"title\")).rlike(\"\\\\d+\\\\s*(oz|lb|ml|kg|inch|ft)\"), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "if \"features_count\" in df_featured.columns:\n",
    "    df_featured = df_featured.withColumn(\n",
    "        \"features_count\", when(col(\"features_count\").isNull(), 0).otherwise(col(\"features_count\"))\n",
    "    )\n",
    "if \"desc_len\" in df_featured.columns:\n",
    "    df_featured = df_featured.withColumn(\n",
    "        \"desc_len\", when(col(\"desc_len\").isNull(), 0).otherwise(col(\"desc_len\"))\n",
    "    )\n",
    "df_featured = df_featured.withColumn(\"log_rating_number\", log1p(col(\"rating_number\")))\n",
    "if \"store_freq\" in df_featured.columns:\n",
    "    df_featured = df_featured.withColumn(\"log_store_freq\", log1p(col(\"store_freq\")))\n",
    "\n",
    "df_featured = df_featured.withColumn(\"label\", log1p(col(\"price\")))\n",
    "\n",
    "\n",
    "stages = []\n",
    "\n",
    "if \"title\" in df_featured.columns:\n",
    "    tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "    stages.append(tokenizer)\n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"title_features\", numFeatures=500)\n",
    "    stages.append(hashingTF)\n",
    "\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"main_category\", \n",
    "    outputCol=\"category_index\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "stages.append(indexer)\n",
    "\n",
    "numeric_cols = [\n",
    "    \"category_index\",\n",
    "    \"average_rating\",\n",
    "    \"log_rating_number\",      \n",
    "    \"rating_popularity\",\n",
    "    \"rating_confidence\",\n",
    "]\n",
    "\n",
    "optional_cols = [\n",
    "    \"title_len\", \"is_premium\", \"is_bundle\", \"has_size\",\n",
    "    \"features_count\", \"desc_len\", \"log_store_freq\"\n",
    "]\n",
    "\n",
    "for col_name in optional_cols:\n",
    "    if col_name in df_featured.columns:\n",
    "        numeric_cols.append(col_name)\n",
    "\n",
    "input_cols = numeric_cols\n",
    "if \"title_features\" in df_featured.columns:\n",
    "    input_cols = numeric_cols + [\"title_features\"]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features_raw\")\n",
    "stages.append(assembler)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\", \n",
    "    outputCol=\"features_vector\",\n",
    "    withStd=True,\n",
    "    withMean=False \n",
    ")\n",
    "stages.append(scaler)\n",
    "\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features_vector\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=30,         \n",
    "    maxDepth=6,          \n",
    "    stepSize=0.05,       \n",
    "    subsamplingRate=0.8, \n",
    "    minInstancesPerNode=10,\n",
    "    seed=42\n",
    ")\n",
    "stages.append(gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 355,713, Test: 88,971\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "train_data, test_data = df_featured.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_data.count():,}, Test: {test_data.count():,}\")\n",
    "\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Results:\n",
      "  R² (Log Scale):  0.0943\n",
      "  R² (Real Price): -0.0215\n",
      "  RMSE:            $22.37\n",
      "  MAE:             $15.18\n",
      "\n",
      " Examples of predictions:\n",
      "+------------------------------------------------------------+------+------------------+-------------------+\n",
      "|                                                       title|real_$|            pred_$|            error_%|\n",
      "+------------------------------------------------------------+------+------------------+-------------------+\n",
      "|          .30-06 Outdoors Premium Parallel Limb Bow Case 41\"| 49.99| 25.99273602864092|  48.00413051428783|\n",
      "|                .30-06 Mustang Compact Camo Release Web Stem| 25.99|29.892387867223743|-15.014960101854927|\n",
      "|                               .30-06 Outdoors K3 Stabilizer| 54.95|21.933703036420997|  60.08425344515375|\n",
      "|                   30-06 Outdoors Alpha Crossbow Case, Black| 94.76| 26.21370614809161|  72.33673959778324|\n",
      "|                       .30-06 10 Ring Paper Target 100 Count| 51.59|26.105859987138903|  49.39744153920231|\n",
      "|281Z Winter Warmer Neck Gaiter - Military Outdoor Sport -...| 17.99| 19.67012782104988|  -9.33923330354378|\n",
      "|             7iDP M1 Helmet Full Face Mountain Biking Helmet| 93.89|22.536013031832766|   75.9974297813049|\n",
      "|AC UNDERCOVER Men's Concealed Carry Tank Top - Compressio...|  32.5|18.788597441790166|  42.18893094833795|\n",
      "|ActionUnion Airsoft PEQ 15 IR Laser PEQ Box White Flashli...| 49.99|24.140359395011046| 51.709624756004935|\n",
      "|ACTIONUNION Airsoft PEQ-15S Pro UHP Visible Blue Laser + ...| 89.99| 27.47097498878797|  69.47330187697678|\n",
      "+------------------------------------------------------------+------+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Important features:\n",
      "   1. category_index            0.3490\n",
      "   2. log_store_freq            0.2100\n",
      "   3. title_len                 0.1406\n",
      "   4. features_count            0.1023\n",
      "   5. average_rating            0.0558\n",
      "   6. log_rating_number         0.0415\n",
      "   7. has_size                  0.0295\n",
      "   8. is_premium                0.0259\n",
      "   9. rating_popularity         0.0242\n",
      "  10. is_bundle                 0.0195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[store: string, parent_asin: string, title: string, main_category: string, average_rating: double, rating_number: bigint, features: array<string>, description: array<string>, price: float, main_category_label: string, features_count: int, title_len: int, store_freq: bigint, rating_popularity: double, rating_confidence: int, is_premium: int, is_bundle: int, has_size: int, log_rating_number: double, log_store_freq: double, label: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "predictions = predictions.withColumn(\"prediction_price\", expm1(col(\"prediction\")))\n",
    "predictions = predictions.withColumn(\"error\", col(\"price\") - col(\"prediction_price\"))\n",
    "predictions = predictions.withColumn(\"error_pct\", \n",
    "    (col(\"error\") / col(\"price\")) * 100\n",
    ")\n",
    "\n",
    "r2_log = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"\n",
    ").evaluate(predictions)\n",
    "rmse = RegressionEvaluator(\n",
    "    labelCol=\"price\", \n",
    "    predictionCol=\"prediction_price\", \n",
    "    metricName=\"rmse\"\n",
    ").evaluate(predictions)\n",
    "\n",
    "mae = RegressionEvaluator(\n",
    "    labelCol=\"price\", \n",
    "    predictionCol=\"prediction_price\", \n",
    "    metricName=\"mae\"\n",
    ").evaluate(predictions)\n",
    "\n",
    "r2_real = RegressionEvaluator(\n",
    "    labelCol=\"price\", \n",
    "    predictionCol=\"prediction_price\", \n",
    "    metricName=\"r2\"\n",
    ").evaluate(predictions)\n",
    "\n",
    "print(\"\\Results:\")\n",
    "print(f\"  R² (Log Scale):  {r2_log:.4f}\")\n",
    "print(f\"  R² (Real Price): {r2_real:.4f}\")\n",
    "print(f\"  RMSE:            ${rmse:.2f}\")\n",
    "print(f\"  MAE:             ${mae:.2f}\")\n",
    "\n",
    "print(\"\\n Examples of predictions:\")\n",
    "predictions.select(\n",
    "    \"title\", \n",
    "    col(\"price\").alias(\"real_$\"), \n",
    "    col(\"prediction_price\").alias(\"pred_$\"),\n",
    "    col(\"error_pct\").alias(\"error_%\")\n",
    ").show(10, truncate=60)\n",
    "\n",
    "\n",
    "gbt_model = model.stages[-1]\n",
    "\n",
    "importances = gbt_model.featureImportances.toArray()\n",
    "feature_names = numeric_cols + ([\"title_features\"] if \"title_features\" in df_featured.columns else [])\n",
    "\n",
    "feature_imp = list(zip(feature_names, importances))\n",
    "feature_imp.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nImportant features:\")\n",
    "for i, (name, importance) in enumerate(feature_imp[:10], 1):\n",
    "    print(f\"  {i:2d}. {name:25s} {importance:.4f}\")\n",
    "\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows which inputs have the strongest impact on price prediction, with category, store frequency, and title length being the most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit ('3.11.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1614c1b0428f9a7744170f6e726f1b37a0202d0ea9f400a7179d4fdaa8b27423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
